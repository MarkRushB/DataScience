# Bayesian methods for determining model parameters

>Reference: 
>- [INFO-6105](./DataScience%20Reference/mcmc-mixture-model-dino-start.ipynb)
>- [æ‰“å¼€MCMCï¼ˆé©¬å°”ç§‘å¤«è’™ç‰¹å¡æ´›ï¼‰çš„é»‘ç›’å­ - Pymcè´å¶æ–¯æŽ¨ç†åº•å±‚å®žçŽ°åŽŸç†åˆæŽ¢
](https://www.cnblogs.com/LittleHann/p/9550757.html#_lab2_1_1)
>- [AI pioneer Sejnowski says itâ€™s all about the gradient](https://www.zdnet.com/article/ai-pioneer-sejnowski-says-its-all-about-the-gradient/)

## The Bayesian expectation fabric

When we create a model with  ð‘  parameters that we want to solve with Baeysian inference, we are implicitly creating an  ð‘  dimensional spaceï¼ˆ**å¯ä»¥ç†è§£ä¸ºNä¸ªéšæœºå˜é‡**ï¼‰for the **prior** distribution of each paramater to live in.

Associated with the space is an **extra dimension**, which we can describe as the surface, or manifold, that sits on top of the space, that reflects the probability of observing data. The surface on the space is defined by our prior distribution and warped by the **data likelihood**. I call this fabric the **Bayesian expectation fabric**.

### å…ˆéªŒåˆ†å¸ƒçš„å¯è§†åŒ–å›¾æ™¯åƒ

æˆ‘ä»¬è¿™é‡Œé€‰æ‹©2ç»´çš„ï¼Œå³åŒ…å«2ä¸ªéšæœºå˜é‡çš„è´å¶æ–¯æŽ¨æ–­é—®é¢˜ï¼Œè¿›è¡Œå¯è§†åŒ–å±•ç¤ºï¼Œé€‰æ‹©2ç»´æ˜¯å› ä¸ºå¯ä»¥æ–¹ä¾¿è¿›è¡Œå¯è§†åŒ–ï¼Œé«˜ç»´ç©ºé—´æ˜¯å¾ˆéš¾æƒ³è±¡çš„ã€‚

**äºŒç»´å‡åŒ€åˆ†å¸ƒçš„å…ˆéªŒå›¾æ™¯åƒ**

For example, if we have two unknown probability distributions  ð‘1  and  ð‘2 , and priors for both are  Uniform(0,5) , the space created is a square of length 5 and the surface is a flat plane that sits on top of the square (representing the concept that every point is equally likely).
```python
%matplotlib inline
import scipy.stats as stats
from IPython.core.pylabtools import figsize
import numpy as np
figsize(12.5, 4)

import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

jet = plt.cm.jet
fig = plt.figure()
x = y = np.linspace(0, 5, 100)
X, Y = np.meshgrid(x, y)

plt.subplot(121)
uni_x = stats.uniform.pdf(x, loc=0, scale=5) #å‡åŒ€åˆ†å¸ƒ
uni_y = stats.uniform.pdf(y, loc=0, scale=5) #å‡åŒ€åˆ†å¸ƒ
M = np.dot(uni_x[:, None], uni_y[None, :])
im = plt.imshow(M, interpolation='none', origin='lower',
                cmap=jet, vmax=1, vmin=-.15, extent=(0, 5, 0, 5))

plt.xlim(0, 5)
plt.ylim(0, 5)
plt.title("Landscape formed by Uniform priors.")

ax = fig.add_subplot(122, projection='3d')
ax.plot_surface(X, Y, M, cmap=plt.cm.jet, vmax=1, vmin=-.15)
ax.view_init(azim=390)
plt.title("Uniform prior landscape; alternate view");
```
![](https://markpersonal.oss-us-east-1.aliyuncs.com/pic/20200311222736.png)

**äºŒç»´æŒ‡æ•°åˆ†å¸ƒçš„å…ˆéªŒå›¾æ™¯åƒ**

Alternatively, if the two priors are exponentials, for example  Exp(3)  and  Exp(10) , then the space is all positive numbers on the 2-D plane, and the surface induced by the priors looks like a **water-fall** that starts at the point (0,0) and flows over the positive numbers. Do you want to see what this looks like? I do.

```python
%matplotlib inline
import scipy.stats as stats
from IPython.core.pylabtools import figsize
import numpy as np
figsize(12.5, 5)

import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

jet = plt.cm.jet
fig = plt.figure()
x = y = np.linspace(0, 5, 100)
X, Y = np.meshgrid(x, y)

fig = plt.figure()
plt.subplot(121)

exp_x = stats.expon.pdf(x, scale=3)
exp_y = stats.expon.pdf(x, scale=10)
M = np.dot(exp_x[:, None], exp_y[None, :])
CS = plt.contour(X, Y, M)
im = plt.imshow(M, interpolation='none', origin='lower',
                cmap=jet, extent=(0, 5, 0, 5))
#plt.xlabel("prior on $p_1$")
#plt.ylabel("prior on $p_2$")
plt.title("$Exp(3), Exp(10)$ prior landscape")

ax = fig.add_subplot(122, projection='3d')
ax.plot_surface(X, Y, M, cmap=jet)
ax.view_init(azim=390)
plt.title("$Exp(3), Exp(10)$ prior landscape; \nalternate view");
```
![](https://markpersonal.oss-us-east-1.aliyuncs.com/pic/20200311223326.png)

The plots below visualize this. The more dark-red the color, the more prior probability is assigned to that location. Conversely, areas with darker blue represent that our priors assign very low probability to that location.ï¼ˆè¶Šçº¢ï¼Œå…ˆéªŒæ¦‚çŽ‡è¶Šé«˜ï¼›è¶Šè“ï¼Œå…ˆéªŒæ¦‚çŽ‡è¶Šä½Žï¼‰

### è§‚æµ‹æ ·æœ¬æ˜¯å¦‚ä½•å½±å“æœªçŸ¥å˜é‡çš„å…ˆéªŒåˆ†å¸ƒçš„ï¼Ÿ

æ¦‚çŽ‡é¢æè¿°äº†æœªçŸ¥å˜é‡çš„å…ˆéªŒåˆ†å¸ƒï¼Œè€Œè§‚æµ‹æ ·æœ¬çš„ä½œç”¨æˆ‘ä»¬å¯ä»¥å½¢è±¡åœ°ç†è§£ä¸ºä¸€åªæ‰‹ï¼Œæ¯æ¬¡æ¥ä¸€ä¸ªæ ·æœ¬ï¼Œè¿™åªæ‰‹å°±æ ¹æ®è§‚æµ‹æ ·æœ¬çš„æƒ…å†µï¼Œå°†å…ˆéªŒåˆ†å¸ƒçš„æ›²é¢å‘â€œç¬¦åˆâ€è§‚æµ‹æ ·æœ¬çš„æ–¹å‘æ‹‰ä¼¸ä¸€ç‚¹ã€‚

åœ¨MCMCè¿‡ç¨‹ä¸­ï¼Œè§‚æµ‹æ ·æœ¬åªæ˜¯é€šè¿‡æ‹‰ä¼¸å’ŒåŽ‹ç¼©å…ˆéªŒåˆ†å¸ƒçš„æ›²é¢ï¼Œè®©æ›²é¢æ›´ç¬¦åˆå®žé™…çš„å‚æ•°åˆ†å¸ƒï¼Œä»¥è¡¨æ˜Žå‚æ•°çš„çœŸå®žå€¼æœ€å¯èƒ½åœ¨å“ªé‡Œã€‚

The data  ð‘‹  changes the surface of the space by pulling and stretching the fabric of the prior surface to reflect where the true parameters likely live. More data means more pulling and stretching, and our original shape becomes mangled or insignificant compared to the newly formed shape. Less data, and our original shape is more present. Regardless, the resulting surface describes the posterior distribution.
æ•°æ®ð‘‹è¶Šå¤šæ‹‰ä¼¸å’ŒåŽ‹ç¼©å°±è¶ŠåŽ‰å®³ï¼Œè¿™æ ·åŽéªŒåˆ†å¸ƒå°±å˜åŒ–çš„è¶ŠåŽ‰å®³ï¼Œå¯èƒ½å®Œå…¨çœ‹ä¸å‡ºå’Œå…ˆéªŒåˆ†å¸ƒçš„æ›²é¢æœ‰ä»€ä¹ˆå…³ç³»ï¼Œæˆ–è€…è¯´éšç€æ•°æ®çš„å¢žåŠ å…ˆéªŒåˆ†å¸ƒå¯¹äºŽåŽéªŒåˆ†å¸ƒçš„å½±å“è¶Šæ¥è¶Šå°ã€‚è¿™ä¹Ÿä½“çŽ°äº†è´å¶æ–¯æŽ¨æ–­çš„æ ¸å¿ƒæ€æƒ³ï¼š**ä½ çš„å…ˆéªŒåº”è¯¥å°½å¯èƒ½åˆç†ï¼Œä½†æ˜¯å³ä½¿ä¸æ˜¯é‚£ä¹ˆçš„åˆç†ä¹Ÿæ²¡æœ‰å¤ªå¤§å…³ç³»ï¼ŒMCMCä¼šé€šè¿‡è§‚æµ‹æ ·æœ¬çš„æ‹Ÿåˆï¼Œå°½å¯èƒ½å°†å…ˆéªŒåˆ†å¸ƒè°ƒæ•´ä¸ºç¬¦åˆè§‚æµ‹æ ·æœ¬çš„åŽéªŒåˆ†å¸ƒã€‚**

ä½†æ˜¯å¦‚æžœæ•°æ®ð‘‹è¾ƒå°ï¼Œé‚£ä¹ˆåŽéªŒåˆ†å¸ƒçš„å½¢çŠ¶ä¼šæ›´å¤šçš„åæ˜ å‡ºå…ˆéªŒåˆ†å¸ƒçš„å½¢çŠ¶ã€‚åœ¨å°æ ·æœ¬çš„æƒ…å†µä¸‹ï¼ŒMCMCå¯¹åˆå§‹çš„å…ˆéªŒåˆ†å¸ƒä¼šéžå¸¸æ•æ„Ÿã€‚

**ä¸åŒçš„å…ˆéªŒæ¦‚çŽ‡å¯¹è§‚æµ‹æ ·æœ¬è°ƒæ•´åŽéªŒåˆ†å¸ƒçš„é˜»åŠ›æ˜¯ä¸åŒçš„**

>**ANALOGY**: Probabilities allowed me to talk about quantum physics with you. Now Bayesian estimation will let me talk about Einstein's general relativity: Evidence warps the space of prior distributions much in the same way that planterary bodies warp space and create the force of gravity through geometry!  

![](https://markpersonal.oss-us-east-1.aliyuncs.com/pic/20200311230052.png)
æ—¶ç©ºæ›²çŽ‡å¼•èµ·é‡åŠ›

For two dimensions, in the opposite of gravity, data essentially pushes up the original surface to make tall mountains. 

éœ€è¦å†æ¬¡è¯´æ˜Žçš„æ˜¯ï¼Œåœ¨é«˜ç»´ç©ºé—´ä¸Šï¼Œæ‹‰ä¼¸å’ŒæŒ¤åŽ‹çš„å˜åŒ–éš¾ä»¥å¯è§†åŒ–ã€‚åœ¨äºŒç»´ç©ºé—´ä¸Šï¼Œè¿™äº›æ‹‰ä¼¸ã€æŒ¤åŽ‹çš„ç»“æžœæ˜¯å½¢æˆäº†å‡ åº§å±±å³°ã€‚è€Œå½¢æˆè¿™äº›å±€éƒ¨å±±å³°çš„ä½œç”¨åŠ›ä¼šå—åˆ°å…ˆéªŒåˆ†å¸ƒçš„é˜»æŒ ã€‚

The tendency of observed data to push up the posterior probability in certain areas is checked by the prior probability distribution, **so that small in magnitude prior probability means more resistance.**

å…ˆéªŒåˆ†å¸ƒè¶Šå°æ„å‘³ç€é˜»åŠ›è¶Šå¤§ï¼›å…ˆéªŒåˆ†å¸ƒè¶Šå¤§é˜»åŠ›è¶Šå°ã€‚

>So priors and data likelhihood compete against each other, the same way your mind competes against two thoughts: "She smokes! I don't like her anymore!" competes with "but she is so pretty!"

æœ‰ä¸€ç‚¹è¦ç‰¹åˆ«æ³¨æ„ï¼Œå¦‚æžœæŸå¤„çš„å…ˆéªŒåˆ†å¸ƒä¸º0ï¼Œé‚£ä¹ˆåœ¨è¿™ä¸€ç‚¹ä¸Šä¹ŸæŽ¨ä¸å‡ºåŽéªŒæ¦‚çŽ‡ã€‚

Suppose the priors mentioned above represent different parameters  ðœ†  of two Poisson distributions. Now, we observe a datapoint and visualize the new landscape. This datapoint is a random variate from a 2D Poisson distribution (think about the distribution as the number of emails you recieve and the number of text messages, in one day). This new data is going to change our priors into posteriors. It is going to warp the Bayesian fabric.
```python
import numpy as np
import matplotlib.pyplot as plt
import pymc as pm
from scipy.stats.mstats import mquantiles
from separation_plot import separation_plot
import scipy.stats as stats

jet = plt.cm.jet
```
```python
# create observed data

# sample size of data we observe, trying varying this (keep it less than 100 ;)
N = 1

# the true parameters, but of course we do not see these values...
lambda_1_true = 1
lambda_2_true = 3

#...we only see the data generated, dependent on the above two values.
data = np.concatenate([
    stats.poisson.rvs(lambda_1_true, size=(N, 1)),
    stats.poisson.rvs(lambda_2_true, size=(N, 1))
], axis=1)

data
```
Now this is called the **data likelihood**, which in each dimension is the probability density function pdf (actually probability **mass** function pmf since our random variable is discrete rather than continuous) of the observed new data, which stems from a Poisson distribution, with expectation equal to the datapoint we observe (not the expectation of our priors, which is different):
```python
# plotting details.
x = y = np.linspace(.01, 5, 100)
likelihood_x = np.array([stats.poisson.pmf(data[:, 0], _x)
                        for _x in x]).prod(axis=1)
likelihood_y = np.array([stats.poisson.pmf(data[:, 1], _y)
                        for _y in y]).prod(axis=1)
L = np.dot(likelihood_x[:, None], likelihood_y[None, :])
```
![](https://markpersonal.oss-us-east-1.aliyuncs.com/pic/20200311231745.png)

First, let's plot our priors: How we think the data will be distributed, before we actually observe any data. We start with **uniform** priors:

```python
uni_x = stats.uniform.pdf(x, loc=0, scale=5)
uni_y = stats.uniform.pdf(y, loc=0, scale=5)
M = np.dot(uni_x[:, None], uni_y[None, :])
im = plt.imshow(M, interpolation='none', origin='lower',
                cmap=jet, vmax=1, vmin=-.15, extent=(0, 5, 0, 5))
plt.xlim(0, 5)
plt.ylim(0, 5)
plt.title("Landscape formed by Uniform priors on $p_1, p_2$.")
```
![](https://markpersonal.oss-us-east-1.aliyuncs.com/pic/20200311231950.png)

Our **posterior**, as given by Bayes' formula (we omit the denominator) is: The **data likelihood** L times the **prior** M. The mountain's peak is the datapoint we observe, because once we observe a datapoint, we expect all other datapoints to appear *at the same location*. We also plot, in yellow, the ***real*** expectation of the data generating mechanism, which we know is a Poisson with expectation (1, 3), and in green the datapoint we observe.
```python
plt.contour(x, y, M * L)
im = plt.imshow(M * L, interpolation='none', origin='lower',
                cmap=jet, extent=(0, 5, 0, 5))
plt.title("Landscape warped by %d data observation;\n Uniform priors on $p_1, p_2$." % N)
plt.scatter(lambda_2_true, lambda_1_true, c="y", s=50, edgecolor="none")
plt.xlim(0, 5)
plt.ylim(0, 5)
```
![](https://markpersonal.oss-us-east-1.aliyuncs.com/pic/20200311232227.png)

Given the new datapoint, we now expect to observe data coming in with probability contours as pictured above: We expect new data to come in with highest probability exactly where the previous datapoint came in.

Now, what if we had started with **exponential** priors instead? In other words, we think there is a ***data fountain*** at the ***origin***, and we expected our data to come from there instead, rather than uniformly. Then our prior becomes:
```python
exp_x = stats.expon.pdf(x, loc=0, scale=3)
exp_y = stats.expon.pdf(x, loc=0, scale=10)
M = np.dot(exp_x[:, None], exp_y[None, :])

plt.contour(x, y, M)
im = plt.imshow(M, interpolation='none', origin='lower',
                cmap=jet, extent=(0, 5, 0, 5))
plt.xlim(0, 5)
plt.ylim(0, 5)
plt.title("Landscape formed by Exponential priors on $p_1, p_2$.")
```
![](https://markpersonal.oss-us-east-1.aliyuncs.com/pic/20200311232426.png)

That is how we would expect data to flow if it flowed from the origin!

Now, we observe a datapoint! Yay! We're not alone in the universe anymore :-)

How does this datapoint modify our posterior M * L?
```python
plt.contour(x, y, M * L)
im = plt.imshow(M * L, interpolation='none', origin='lower',
                cmap=jet, extent=(0, 5, 0, 5))

plt.scatter(lambda_2_true, lambda_1_true, c="y", s=50, edgecolor="none")
plt.title("Landscape warped by %d data observation;\n Exponential priors on \
$p_1, p_2$." % N)
plt.xlim(0, 5)
plt.ylim(0, 5);
```
![](https://markpersonal.oss-us-east-1.aliyuncs.com/pic/20200311232733.png)

We see that our datapoint created a mountain peak in our expectation fabric, just like before,but more *squished*. The top of that mountain is not the datapoint itself because our prior was a data fountain at the origin, it *fights* with our datapoint and the result of that tectonic shift is the landscape you see pictured above.

Let's bring this all together into a *combined* plot, and introduce more datapoints! It's like playing God with the tectonic plates on earth! 

We color in red our datapoints, in green the expectation which is nothing more than the $\lambda$s of our Poissons. You should be able to see that as you increase the number of datapoints, your posterior gets tighter and tighter (and taller and taller) and the peak gets closer and closer to the theoretical expecation, which is where the green point lies.

The more data we observe (from a theoretical Poission distribution will well know Expectation), the tighter and tighter our mountain centered around that expectation. We've already seen this phenomenon with coin tosses!
```python
import numpy as np
import matplotlib.pyplot as plt
import pymc as pm
from scipy.stats.mstats import mquantiles
from separation_plot import separation_plot
import scipy.stats as stats

jet = plt.cm.jet

#
# create observed data
#

# sample size of data we observe, trying varying this (keep it less than 100 ;)
N = 1

# the true parameters, but of course we do not see these values...
lambda_1_true = 1
lambda_2_true = 3

#...we see the data generated, dependent on the above two values.
data = np.concatenate([
    stats.poisson.rvs(lambda_1_true, size=(N, 1)),
    stats.poisson.rvs(lambda_2_true, size=(N, 1))
], axis=1)
print(data)

#
# Likelihood function fron the observed data
#

x = y = np.linspace(.01, 5, 100)
likelihood_x = np.array([stats.poisson.pmf(data[:, 0], _x)
                        for _x in x]).prod(axis=1)
likelihood_y = np.array([stats.poisson.pmf(data[:, 1], _y)
                        for _y in y]).prod(axis=1)
L = np.dot(likelihood_x[:, None], likelihood_y[None, :])

#
# plots
#

figsize(12.5, 12)
plt.subplot(221)
uni_x = stats.uniform.pdf(x, loc=0, scale=5)
uni_y = stats.uniform.pdf(x, loc=0, scale=5)
M = np.dot(uni_x[:, None], uni_y[None, :])
im = plt.imshow(M, interpolation='none', origin='lower',
                cmap=jet, vmax=1, vmin=-.15, extent=(0, 5, 0, 5))
plt.xlim(0, 5)
plt.ylim(0, 5)
plt.title("Landscape formed by Uniform priors on $p_1, p_2$.")

plt.subplot(223)
plt.contour(x, y, M * L)
im = plt.imshow(M * L, interpolation='none', origin='lower',
                cmap=jet, extent=(0, 5, 0, 5))
plt.title("Landscape warped by %d data observation;\n Uniform priors on $p_1, p_2$." % N)
plt.scatter(lambda_2_true, lambda_1_true, c="g", s=50, edgecolor="none")
for _ in range(N):
    plt.scatter(data[_][0], data[_][1], c="r", s=50, edgecolor="none")
plt.xlim(0, 5)
plt.ylim(0, 5)

plt.subplot(222)
exp_x = stats.expon.pdf(x, loc=0, scale=3)
exp_y = stats.expon.pdf(x, loc=0, scale=10)
M = np.dot(exp_x[:, None], exp_y[None, :])

plt.contour(x, y, M)
im = plt.imshow(M, interpolation='none', origin='lower',
                cmap=jet, extent=(0, 5, 0, 5))
plt.xlim(0, 5)
plt.ylim(0, 5)
plt.title("Landscape formed by Exponential priors on $p_1, p_2$.")

plt.subplot(224)
# This is the likelihood times prior, that results in the posterior.
plt.contour(x, y, M * L)
im = plt.imshow(M * L, interpolation='none', origin='lower',
                cmap=jet, extent=(0, 5, 0, 5))

plt.scatter(lambda_2_true, lambda_1_true, c="g", s=50, edgecolor="none")
for _ in range(N):
    plt.scatter(data[_][0], data[_][1], c="r", s=50, edgecolor="none")
plt.title("Landscape warped by %d data observation;\n Exponential priors on \
$p_1, p_2$." % N)
plt.xlim(0, 5)
plt.ylim(0, 5);
```
![](https://markpersonal.oss-us-east-1.aliyuncs.com/pic/20200311234027.png)

The highest points on the mountain, corresponding the the darkest red, are biased towards (0,0) in the exponential case, which is the result from the exponential prior putting more prior weight in the (0,0) corner.

The green dot represents the true parameters, or **expectation**, and ***as we increase the number of datapoints, our mountain peak gets closer and closer to the expectation***.

å››å¼ å›¾é‡Œçš„ç»¿ç‚¹ä»£è¡¨å‚æ•°çš„çœŸå®žå–å€¼ã€‚æ¯åˆ—çš„ä¸Šå›¾è¡¨ç¤ºå…ˆéªŒåˆ†å¸ƒå›¾å½¢ï¼Œä¸‹å›¾è¡¨ç¤ºåŽéªŒåˆ†å¸ƒå›¾å½¢ã€‚

æˆ‘ä»¬ä¸»è¦åˆ°ï¼Œè™½ç„¶è§‚æµ‹å€¼ç›¸åŒï¼Œä¸¤ç§å…ˆéªŒå‡è®¾ä¸‹å¾—åˆ°çš„åŽéªŒåˆ†å¸ƒå´æœ‰ä¸åŒçš„å›¾å½¢ã€‚

æˆ‘ä»¬ä»Žä¸Šå›¾é‡Œæ³¨æ„åˆ°2ä¸ªç»†èŠ‚ï¼š

1. å³ä¸‹æ–¹çš„æŒ‡æ•°å…ˆéªŒå¯¹åº”çš„åŽéªŒåˆ†å¸ƒå›¾å½¢ä¸­ï¼Œå³ä¸Šè§’åŒºåŸŸçš„å–å€¼å¾ˆä½Žï¼ŒåŽŸå› æ˜¯å‡è®¾çš„æŒ‡æ•°å…ˆéªŒåœ¨è¿™ä¸€åŒºåŸŸçš„å–å€¼ä¹Ÿè¾ƒä½Žã€‚
2. å¦ä¸€æ–¹é¢ï¼Œå·¦ä¸‹æ–¹çš„å‡åŒ€åˆ†å¸ƒå¯¹åº”çš„åŽéªŒåˆ†å¸ƒå›¾å½¢ä¸­ï¼Œå³ä¸Šè§’åŒºåŸŸçš„å–å€¼ç›¸å¯¹è¾ƒé«˜ï¼Œè¿™ä¹Ÿæ˜¯å› ä¸ºå‡åŒ€å…ˆéªŒåœ¨è¯¥åŒºåŸŸçš„å–å€¼ç›¸æ¯”æŒ‡æ•°å…ˆéªŒæ›´é«˜ã€‚
3. åœ¨å³ä¸‹è§’æŒ‡æ•°åˆ†å¸ƒçš„åŽéªŒå›¾å½¢ä¸­ï¼Œæœ€é«˜çš„å±±å³°ï¼Œä¹Ÿå°±æ˜¯çº¢è‰²æœ€æ·±çš„åœ°æ–¹ï¼Œå‘ï¼ˆ0ï¼Œ0ï¼‰ç‚¹åæ–œï¼ŒåŽŸå› å°±æ˜¯æŒ‡æ•°å…ˆéªŒåœ¨è¿™ä¸ªè§’è½çš„å–å€¼æ›´é«˜ã€‚

>æ€è€ƒï¼šè¿™ä¸ªçŽ°è±¡å…¶å®žå’Œæ·±åº¦å­¦ä¹ é‡Œsigmoidå‡½æ•°çš„è°ƒæ•´è¿‡ç¨‹æ˜¯ç±»ä¼¼çš„ï¼Œsigmoidåœ¨è¶Šé è¿‘0æˆ–1æ¦‚çŽ‡çš„åŒºåŸŸä¸­ï¼Œè°ƒæ•´çš„é€ŸçŽ‡ä¼šè¶Šæ¥è¶Šæ…¢ï¼Œå³æ­»èƒ¡åŒæ•ˆåº”ã€‚å› ä¸ºè¿™æ—¶å€™sigmoidä¼šè®¤ä¸ºæ”¶æ•›å·²ç»æŽ¥è¿‘å°¾å£°ï¼Œè¦å‡ç¼“è°ƒæ•´çš„å¹…åº¦ï¼Œä»¥ç¨³å®šå·²ç»æ”¶æ•›åˆ°çš„ç»“æžœã€‚

Try changing the sample size to other values (try 5, 10, 100) and observe how the mountain posterior changes.

That is the essence of Bayesian dynamics.

### ä½¿ç”¨ PyMC3 MCMCæ¥æœç´¢å›¾æ™¯åƒ

Exploring the deformed posterior space generated by our prior surface and observed data is a great exercise. However, we cannot naively search the space: Traversing $N$-dimensional space is exponentially difficult in $N$: the size of the space quickly blows-up as we increase $N$ ([the curse of dimensionality](http://en.wikipedia.org/wiki/Curse_of_dimensionality)).

How do we find these hidden mountains? The idea behind Markov Chain Monte Carlo algorithms is to perform an ***intelligent search*** of the space. 
æœç´¢ä»€ä¹ˆå‘¢ï¼Ÿä¸€ä¸ªç‚¹å—ï¼Ÿè‚¯å®šä¸æ˜¯ï¼Œè´å¶æ–¯æ€æƒ³çš„æ ¸å¿ƒå°±æ˜¯ä¸–ç•Œä¸Šæ²¡æœ‰100%ç¡®å®šçš„ä¸œè¥¿ï¼Œæ‰€æœ‰çš„æŽ¨æ–­éƒ½æ˜¯ä¸€ä¸ªæ¦‚çŽ‡åˆ†å¸ƒã€‚

MCMC algorithms like **Metropolis** return **samples** from the posterior distribution, not the distribution itself. 

MCMC performs a task similar to repeatedly asking "*How likely is this pebble I found to be from the mountain I am searching for*?", and completes its task by returning thousands of accepted pebbles in hopes of reconstructing the original mountain. In MCMC and PyMC3 lingo, the returned sequence of "*pebbles*" are the **samples**, cumulatively called the **traces**. 
åœ¨MCMCå’ŒPyMCçš„æœ¯è¯­é‡Œï¼Œè¿™äº›è¿”å›žåºåˆ—é‡Œçš„â€œçŸ³å¤´â€å°±æ˜¯è§‚æµ‹æ ·æœ¬ï¼Œç´¯è®¡èµ·æ¥ç§°ä¹‹ä¸ºâ€œè¿¹â€ã€‚

æˆ‘ä»¬å¸Œæœ›MCMCæœç´¢çš„ä½ç½®èƒ½æ”¶æ•›åˆ°åŽéªŒæ¦‚çŽ‡æœ€é«˜çš„åŒºåŸŸï¼ˆæ³¨æ„ä¸æ˜¯ä¸€ä¸ªç¡®å®šçš„ç‚¹ï¼Œæ˜¯ä¸€ä¸ªåŒºåŸŸï¼‰ã€‚ä¸ºæ­¤ï¼ŒMCMCæ¯æ¬¡éƒ½ä¼šæŽ¢ç´¢é™„è¿‘ä½ç½®ä¸Šçš„æ¦‚çŽ‡å€¼ï¼Œå¹¶æœç€æ¦‚çŽ‡å€¼å¢žåŠ çš„æ–¹å‘å‰è¿›ã€‚
MCMC does this by exploring nearby positions and moving into areas with higher probability, picking up samples from that area.

**Why do we pick up thousands of samples?**

æˆ‘ä»¬å¯èƒ½ä¼šè¯´ï¼Œç®—æ³•æ¨¡åž‹è®­ç»ƒçš„ç›®çš„ä¸å°±æ˜¯ä¸ºäº†èŽ·å¾—æˆ‘ä»¬å¯¹éšæœºå˜é‡çš„æœ€ä¼˜ä¼°è®¡å—ï¼Ÿæ¯•ç«Ÿåœ¨å¾ˆå¤šæ—¶å€™ï¼Œæˆ‘ä»¬è®­ç»ƒå¾—åˆ°çš„æ¨¡åž‹ä¼šç”¨äºŽä¹‹åŽçš„é¢„æµ‹ä»»åŠ¡ã€‚ä½†æ˜¯è´å¶æ–¯å­¦æ´¾ä¸è¿™ä¹ˆåšï¼Œè´å¶æ–¯æŽ¨æ–­çš„ç»“æžœæ›´åƒæ˜¯ä¸€ä¸ªå‚è°‹ï¼Œå®ƒåªæä¾›ä¸€ä¸ªå»ºè®®ï¼Œè€Œæœ€åŽçš„å†³ç­–éœ€è¦æˆ‘ä»¬è‡ªå·±æ¥å®Œæˆï¼ˆä¾‹å¦‚æˆ‘ä»¬é€šè¿‡å–åŽéªŒä¼°è®¡çš„å‡å€¼ä½œä¸ºæœ€å¤§åŽéªŒä¼°è®¡çš„ç»“æžœï¼‰

å›žåˆ°MCMCçš„è®­ç»ƒè¿”å›žç»“æžœï¼Œå®ƒè¿”å›žæˆåƒä¸Šä¸‡çš„æ ·æœ¬è®©äººè§‰å¾—è¿™æ˜¯ä¸€ç§ä½Žæ•ˆçš„æè¿°åŽéªŒæ¦‚çŽ‡çš„æ–¹å¼ã€‚å®žé™…ä¸Šè¿™æ˜¯ä¸€ç§éžå¸¸é«˜æ•ˆçš„æ–¹æ³•ã€‚ä¸‹é¢æ˜¯å…¶å®ƒå¯èƒ½ç”¨äºŽè®¡ç®—åŽéªŒæ¦‚çŽ‡çš„æ–¹æ³•

1. ç”¨è§£æžè¡¨è¾¾å¼æè¿°â€œå±±å³°åŒºåŸŸâ€(åŽéªŒåˆ†å¸ƒ)ï¼Œè¿™éœ€è¦æè¿°å¸¦æœ‰å±±å³°å’Œå±±è°·çš„ N ç»´æ›²é¢ã€‚åœ¨æ•°å­¦ä¸Šæ˜¯æ¯”è¾ƒå›°éš¾çš„ã€‚
2. ä¹Ÿå¯ä»¥è¿”å›žå›¾å½¢é‡Œçš„é¡¶å³°ï¼Œè¿™ç§æ–¹æ³•åœ¨æ•°å­¦ä¸Šæ˜¯å¯è¡Œçš„ï¼Œä¹Ÿå¾ˆå¥½ç†è§£ï¼ˆè¿™å¯¹åº”äº†å…³äºŽæœªçŸ¥é‡çš„ä¼°è®¡é‡Œæœ€å¯èƒ½çš„å–å€¼ï¼‰ï¼Œä½†æ˜¯è¿™ç§åšæ³•å¿½ç•¥äº†å›¾å½¢çš„å½¢çŠ¶ï¼Œè€Œæˆ‘ä»¬çŸ¥é“ï¼Œåœ¨ä¸€äº›åœºæ™¯ä¸‹ï¼Œè¿™äº›å½¢çŠ¶å¯¹äºŽåˆ¤å®šæœªçŸ¥å˜é‡çš„åŽéªŒæ¦‚çŽ‡æ¥è¯´ï¼Œæ˜¯éžå¸¸å…³é”®çš„
3. é™¤äº†è®¡ç®—åŽŸå› ï¼Œå¦ä¸€ä¸ªä¸»è¦åŽŸå› æ˜¯ï¼Œåˆ©ç”¨è¿”å›žçš„å¤§é‡æ ·æœ¬å¯ä»¥åˆ©ç”¨å¤§æ•°å®šç†è§£å†³éžå¸¸æ£˜æ‰‹é—®é¢˜ã€‚æœ‰äº†æˆåƒä¸Šä¸‡çš„æ ·æœ¬ï¼Œå°±å¯ä»¥åˆ©ç”¨ç›´æ–¹å›¾æŠ€æœ¯ï¼Œé‡æž„åŽéªŒåˆ†å¸ƒæ›²é¢ã€‚

**MCMC algorithms**

There is a large family of algorithms that perform MCMC, not just Metropolis. Most of these algorithms can be expressed at a high level as follows:

1. Start at current position.
2. Propose moving to a new position (investigate a pebble near you).
3. Accept/Reject the new position based on the position's adherence to the data and prior distributions (ask if the pebble likely came from the mountain).
4.  A.  If you accept: Move to the new position. Return to Step 1.
    B. Else: Do not move to new position. Return to Step 1. 
5. After a large number of iterations, return all accepted positions.

This way we move in the general direction towards regions where the posterior distributions live, and collect samples sparingly on the journey. Once we reach the posterior distribution, we can easily collect samples as they likely all belong to the posterior distribution. 

If the current position of the MCMC algorithm is in an area of extremely low probability, which is often the case when the algorithm begins (typically at a random location in the space), the algorithm will move in positions *that are likely not from the posterior* but better than everything else nearby. Thus the first moves of the algorithm are not reflective of the posterior. That is why we often *throw away* the first samples.

In the above algorithm, notice that only the current position matters (new positions are investigated only near the current position). This property as *memorylessness*, i.e. the algorithm does not care *how* it arrived at its current position, only that it is there. That is why the chain is **Markovian**.

### ä¸€ä¸ªMCMCæœç´¢å›¾æ™¯åƒçš„å®žé™…çš„ä¾‹å­
Unsupervised Clustering using a Mixture Modelï¼ˆä½¿ç”¨æ··åˆæ¨¡åž‹è¿›è¡Œæ— ç›‘ç£èšç±»ï¼‰
```python
import numpy as np
import matplotlib.pyplot as plt
import pymc as pm
import scipy.stats as stats

jet = plt.cm.jet

figsize(12.5, 4)
data = np.loadtxt("mixture_data.csv", delimiter=",")

plt.hist(data, bins=20, color="b", histtype="stepfilled", alpha=0.8)
plt.title("Histogram of the dataset")
plt.ylim([0, None]);
print(data[:10], "...")
```
![](https://markpersonal.oss-us-east-1.aliyuncs.com/pic/20200312001809.png)

It appears the data has a **bimodal** form, that is, it appears to have ***two peaks***, one near 120 and the other near 200. Perhaps there are *two clusters* within this dataset.
**èšç±»æ˜¯ä¸€ä¸ªå¾ˆå®½æ³›çš„æ¦‚å¿µï¼Œä¸ä¸€å®šä»…é™äºŽæˆ‘ä»¬æ‰€ç†Ÿæ‚‰çš„æ¬§å¼ç©ºé—´çš„kmeansï¼Œå®žé™…ä¸Šï¼Œèšç±»ä¸ä¸€å®šéƒ½æ˜¯å‡ ä½•æ„ä¹‰ä¸Šçš„èšç±»ã€‚é€šè¿‡å¯¹åŽŸå§‹æ•°æ®é›†è¿›è¡Œæ¦‚çŽ‡åˆ†å¸ƒçš„æ‹Ÿåˆï¼Œä»Žè€ŒèŽ·å¾—æ•°æ®é›†ä¸­æ¯ä¸ªç‚¹æ‰€å±žç±»åˆ«çš„æ¦‚çŽ‡åˆ†å¸ƒï¼Œè¿™ä¹Ÿæ˜¯ä¸€ç§èšç±»ã€‚**

**1. é€‰æ‹©ç¬¦åˆæ•°æ®è§‚æµ‹åˆ†å¸ƒçš„æ•°å­¦æ¨¡åž‹**
1. For each data point, choose cluster 1 with probability $p$, else choose cluster 2. 
2. Draw a random variate from a Normal distribution with parameters $\mu_i$ and $\sigma_i$ where $i$ was chosen in step 1.
3. Repeat.

è¿™ä¸ªç®—æ³•å¯ä»¥äº§ç”Ÿä¸Žè§‚æµ‹æ•°æ®ç›¸ä¼¼çš„æ•ˆæžœã€‚æ‰€ä»¥é€‰æ‹©è¿™ä¸ªç®—æ³•ä½œä¸ºæ¨¡åž‹ã€‚

ä½†æ˜¯çŽ°åœ¨çš„é—®é¢˜æ˜¯æˆ‘ä»¬ä¸çŸ¥é“å‚æ•° ð‘ å’Œæ­£æ€åˆ†å¸ƒçš„å‚æ•°ã€‚æ‰€ä»¥è¦å­¦ä¹ æˆ–è€…æŽ¨æ–­å‡ºè¿™äº›æœªçŸ¥å˜é‡ã€‚

ç”¨Nor0ï¼ŒNor1åˆ†åˆ«è¡¨ç¤ºæ­£æ€åˆ†å¸ƒã€‚ä¸¤ä¸ªæ­£æ€åˆ†å¸ƒçš„å‚æ•°éƒ½æ˜¯æœªçŸ¥çš„ï¼Œå‚æ•°åˆ†åˆ«è¡¨ç¤ºä¸ºðœ‡ð‘–,ðœŽð‘–ï¼Œð‘– = 0ï¼Œ1ã€‚
**2. å¯¹æ¨¡åž‹çš„å‚æ•°è¿›è¡Œå…ˆéªŒå»ºæ¨¡**

**æ‰€å±žç±»åˆ«åˆ†å¸ƒå…ˆéªŒ**
Denote the Normal distributions $\text{N}_0$ and $\text{N}_1$. Both currently have *unknown* **mean** and **standard deviation**, denoted $\mu_i$ and $\sigma_i, \; i =0,1$ respectively. A specific data point can be from either $\text{N}_0$ or $\text{N}_1$, and we assume that the data point is assigned to $\text{N}_0$ with probability $p$, to $\text{N}_1$ with probability $1-p$.
å¯¹äºŽæŸä¸€ä¸ªå…·ä½“çš„æ•°æ®ç‚¹æ¥è¯´ï¼Œå®ƒå¯èƒ½æ¥è‡ªNor0ä¹Ÿå¯èƒ½æ¥è‡ªNor1ï¼Œ å‡è®¾æ•°æ®ç‚¹æ¥è‡ªNor0çš„æ¦‚çŽ‡ä¸ºð‘ã€‚ è¿™æ˜¯ä¸€ä¸ªå…ˆéªŒï¼Œç”±äºŽæˆ‘ä»¬å¹¶ä¸çŸ¥é“æ¥è‡ª Nor1 çš„å®žé™…æ¦‚çŽ‡ï¼Œå› æ­¤æˆ‘ä»¬åªèƒ½ç”¨ 0-1 ä¸Šçš„å‡åŒ€åˆ†å¸ƒæ¥è¿›è¡Œå»ºæ¨¡å‡è®¾ï¼ˆæœ€å¤§ç†µåŽŸç†ï¼‰ã€‚æˆ‘ä»¬ç§°è¯¥å…ˆéªŒä¸º pã€‚

æœ‰ä¸€ç§è¿‘ä¼¼çš„æ–¹æ³•ï¼Œå¯ä»¥ä½¿ç”¨ PyMC çš„ç±»åˆ«(Categorical)éšæœºå˜é‡å°†æ•°æ®ç‚¹åˆ†é…ç»™æŸä¸€ç±»åˆ«ã€‚PyMC ç±»åˆ«éšæœºå˜é‡æœ‰ä¸€ä¸ªð‘˜ç»´æ¦‚çŽ‡æ•°ç»„å˜é‡ï¼Œå¿…é¡»å¯¹ð‘˜ç»´æ¦‚çŽ‡æ•°ç»„å˜é‡è¿›è¡Œ æ±‚å’Œä½¿å…¶å’Œå˜æˆ 1ï¼ŒPyMC ç±»åˆ«éšæœºå˜é‡çš„ value å±žæ€§æ˜¯ä¸€ä¸ª 0 åˆ°ð‘˜ âˆ’ 1çš„å€¼ï¼Œè¯¥å€¼å¦‚ä½•é€‰ æ‹©ç”±æ¦‚çŽ‡æ•°ç»„ä¸­çš„å…ƒç´ å†³å®š(åœ¨æœ¬ä¾‹ä¸­ð‘˜ = 2)ã€‚

*A priori*, we do not know what the probability of assignment to cluster 1 is, so we form a uniform variable on $(0, 1)$. We call call this $p_1$. The probability of belonging to cluster 2 is therefore $p_2 = 1 - p_1$. Note we should not use a normal variable, because that presupposes an expectation of 0.5, however in this case we ***have no expectation*** for each datapoint!
ç›®å‰è¿˜ä¸çŸ¥é“å°†æ•°æ®åˆ†é…ç»™ç±»åˆ« 1 çš„ å…ˆéªŒæ¦‚çŽ‡æ˜¯å¤šå°‘ï¼Œæ‰€ä»¥é€‰æ‹© 0 åˆ° 1 çš„å‡åŒ€éšæœºå˜é‡ä½œä¸ºå…ˆéªŒåˆ†å¸ƒã€‚æ­¤æ—¶è¾“å…¥ç±»åˆ«å˜é‡çš„ æ¦‚çŽ‡æ•°ç»„ä¸º[ð‘, 1 âˆ’ ð‘]ã€‚

>**NOTE!!:**
>Unfortunately, we can't we just give `[p1, p2]` to our `Categorical` variable. PyMC3 uses `Theano` under the hood to build the models so we need to use `theano.tensor.stack()` to combine $p_1$ and $p_2$ into a vector that it can understand. We pass this vector into the `Categorical` variable as well as the `testval` parameter to give our variable an idea of where to start from: 300 random `0`s and `1`s for each of our 300 datapoints, indicating they belong to either cluster 0 or cluster 1.

```python
import pymc3 as pm
import theano.tensor as T

with pm.Model() as model:
    p1 = pm.Uniform('p', 0, 1)
    p2 = 1 - p1
    p = T.stack([p1, p2])
    
    # this produces worse results. Why? This looks like a categorical assignment with Bernoulli probability
    #assignment = pm.Categorical('assignment', np.array([0.5, 0.5]), 
    #                            shape = data.shape[0],
    #                            testval = np.random.randint(0, 2, data.shape[0]))
    
    # This is better: This looks like a categorical assignment with Dirichlet probabilty 
    assignment = pm.Categorical("assignment", p, 
                                shape = data.shape[0],
                                testval = np.random.randint(0, 2, data.shape[0]))
    
print("prior assignment, with p = %.2f:" % p1.tag.test_value)
print(assignment.tag.test_value[:100])
```
![](https://markpersonal.oss-us-east-1.aliyuncs.com/pic/20200313114538.png)

**å•ä¸ªèšç±»ä¸­çš„æ­£æ€åˆ†å¸ƒçš„å‚æ•°åˆ†å¸ƒå…ˆéªŒ**
Looking at my dataset histogram, I would guess that the standard deviations of the two Normal distributions representing each cluster are different. To maintain ignorance of what the standard deviations might be, we will initially model them as uniform on 0 to 100. We will include both standard deviations in our model using a single line of PyMC3 code:

    sds = pm.Uniform("sds", 0, 100, shape=2)

Notice that we specified `shape=2`: we are modeling both $\sigma$s as a *single* PyMC3 variable. Note that this does not induce a necessary relationship between the two $\sigma$s, it is simply for succinctness. You could have picked two different variables.

We also need to specify priors on the *centers* of the clusters. The centers are really the $\mu$ parameters in these Normal distributions. Their priors can be modeled by a Normal distribution because, looking at the data, I have somewhat of an idea where the two centers might be &mdash; I would guess somewhere around 120 and 190 respectively, though I am not very confident in these eyeballed estimates. Hence I will set $\mu_0 = 120, \mu_1 = 190$ and $\sigma_0 = \sigma_1 = 10$.
ï¼ˆè™½ç„¶æ˜¯è‚‰çœ¼è§‚å¯Ÿåˆ°çš„ï¼Œä½†æ˜¯ä»Žæ•°æ®å½¢çŠ¶ä¸Šæ¥çœ‹ï¼Œæ˜¯åœ¨120å’Œ190é™„è¿‘ï¼Œæœ€é‡è¦çš„æ˜¯ï¼šMCMCä¼šå¸®åŠ©æˆ‘ä»¬ä¿®æ­£å…ˆéªŒä¸­ä¸æ˜¯é‚£ä¹ˆç²¾ç¡®çš„éƒ¨åˆ†ï¼‰

I will also assign a **deterministic** (the opposite of probabilistic) variable to each **probabilistic** `sds` and `centers` variable.

Let's do all this!

- `sds` is a Uniform distribution in[0, 100], in 2 dimensions (of shape 2)
- `centers` is a Normal distribution with mean 120 and standard deviation 10 in one dimension, and mean 120 and standard deviation 10 in the other dimension
- `center_i` is a (deterministic) array of **values** that changes with each `assignment` variable. When assignment is = 0, it denotes `centers[0]`, and when assignment is = 1, it denotes `centers[1]`. So the length of `center_i` is 300
- `sd_i` is a (deterministic) array of **values** that changes with each `assignment` variable. When assignment is = 0, it denotes `sds[0]`, and when assignment is = 1, it denotes `sds[1]`. So the length of `sd_i` is 300
- `observations` is a probabilistic Normal distribution that ***fits*** the observed `data`, with mean and standard deviation that changes for each datapoint in `data`. So sometimes it's `centers[0]` and `sds[0]`, and other times it's `centers[1]` and `sds[1]`, depending on the `assignment` value of each datapoint

```python
with model:
    sds = pm.Uniform("sds", 0, 100, shape=2)
    centers = pm.Normal("centers", 
                        mu=np.array([120, 190]), 
                        sd=np.array([10, 10]), 
                        shape=2)
    
    center_i = pm.Deterministic('center_i', centers[assignment])
    sd_i = pm.Deterministic('sd_i', sds[assignment])
    
    # and to combine it with the observations:
    observations = pm.Normal("obs", mu=center_i, sd=sd_i, observed=data)
    
print("Random assignments: ", assignment.tag.test_value[:100], "...")
print("Assigned center: ", center_i.tag.test_value[:100], "...")
print("Assigned standard deviation: ", sd_i.tag.test_value[:100])
```
![](https://markpersonal.oss-us-east-1.aliyuncs.com/pic/20200313115830.png)

**3. MCMCæœç´¢è¿‡ç¨‹ - è¿¹**

Notice how we continue to build the model within the context of `Model()`. This automatically adds the variables that we create to our model. As long as we work within this context we will be working with the same variables that we have already defined. Any sampling that we do within the context of `Model()` will be done only on the model in which we are working. 

We will tell our model to explore the space that we have so far defined by defining the sampling methods, in this case **`Metropolis()` for our continuous variables and `ElemwiseCategorical()` for our categorical variable (Metropolis does not work for discrete variables).**

We will use these sampling methods to explore the space by using `sample(iterations, step)`, where `iterations` is the number of steps we wish the algorithm to perform and `step` is the way in which we want to handle those steps. 

We use our combination of `Metropolis()` and `ElemwiseCategorical()` for the `step` and sample 25,000 iterations:

>**Important to understand (repeat this 10 times)**: We *sample* from the posterior at every time step of our iteration, and this gives us different samples of all our model paramters. At each time step we get closer and closer to pdfs for our model parameters (`mu` and `sd`) that yield the data likelihood `observations` pdf which fits our empirical histogram the best. So when we converge, we have the exact modeling solution for our empirical data as model **distributions**, yeilding best point approximations as well as errors made (standard deviation).

>**NOTE**: We are going back in time to find the process that generated the data.
```python
with model:
    step1 = pm.Metropolis(vars=[p, sds, centers])
    step2 = pm.ElemwiseCategorical(vars=[assignment])
    trace = pm.sample(25000, step=[step1, step2])
```
>è¿™ä¸€æ­¥æˆ‘çš„mbpéœ€è¦è·‘ä¸¤åˆ†åŠ

We have stored the paths of all our variables, or **traces**, in the `trace` variable. These paths are the routes the unknown parameters (centers, precisions, and $p$) have taken thus far in our exploration of our state space.

The individual path of each variable is indexed by the PyMC3 variable `name` that we gave that variable when defining it within our model. For example, `trace["sds"]` will return a `numpy array` object that we can then index and slice as we would any other `numpy array` object. 
```python
figsize(12.5, 9)
plt.subplot(311)
lw = 1
center_trace = trace["centers"]

# pretty colors
colors = ["#348ABD", "#A60628"] if center_trace[-1, 0] > center_trace[-1, 1] \
    else ["#A60628", "#348ABD"]

plt.plot(center_trace[:, 0], label="trace of center 0", c=colors[0], lw=lw)
plt.plot(center_trace[:, 1], label="trace of center 1", c=colors[1], lw=lw)
plt.title("Traces of unknown parameters")
leg = plt.legend(loc="upper right")
leg.get_frame().set_alpha(0.7)

plt.subplot(312)
std_trace = trace["sds"]
plt.plot(std_trace[:, 0], label="trace of standard deviation of cluster 0",
     c=colors[0], lw=lw)
plt.plot(std_trace[:, 1], label="trace of standard deviation of cluster 1",
     c=colors[1], lw=lw)
plt.legend(loc="upper left")

plt.subplot(313)
p_trace = trace["p"]
plt.plot(p_trace, label="$p$: frequency of assignment to cluster 0",
     color=colors[0], lw=lw)
plt.xlabel("Steps")
plt.ylim(0, 1)
plt.legend();
```
![](https://markpersonal.oss-us-east-1.aliyuncs.com/pic/20200313164604.png)

ä»Žä¸Šå›¾æˆ‘ä»¬å¯ä»¥çœ‹å‡ºä»€ä¹ˆï¼Ÿ

1. The traces converge, not to a single point, but to a *distribution* of possible points. This is *convergence* in an MCMC algorithm!
>è¿™äº›è¿¹å¹¶éžæ”¶æ•›åˆ°æŸä¸€ç‚¹ï¼Œè€Œæ˜¯æ”¶æ•›åˆ°ä¸€å®šåˆ†å¸ƒä¸‹ï¼Œæ¦‚çŽ‡è¾ƒå¤§çš„ç‚¹é›†ã€‚è¿™å°±æ˜¯MCMCç®—æ³•é‡Œæ”¶æ•›çš„æ¶µä¹‰ã€‚

2. Inference using the first few thousand points is a bad idea, as they are unrelated to the final distribution we are interested in. Thus is it a good idea to discard those samples before using the samples for inference. We call this period before converge the *burn-in period*.
>æœ€åˆçš„å‡ åƒä¸ªç‚¹ï¼ˆè®­ç»ƒè½®ï¼‰ä¸Žæœ€ç»ˆçš„ç›®æ ‡åˆ†å¸ƒå…³ç³»ä¸å¤§ï¼Œæ‰€ä»¥ä½¿ç”¨è¿™äº›ç‚¹å‚ä¸Žä¼°è®¡å¹¶ä¸æ˜Žæ™ºã€‚ä¸€ä¸ªèªæ˜Žçš„åšæ³•æ˜¯å‰”é™¤è¿™äº›ç‚¹ä¹‹åŽå†æ‰§è¡Œä¼°è®¡ï¼Œäº§ç”Ÿè¿™äº›é—å¼ƒç‚¹çš„è¿‡ç¨‹ç§°ä¸ºé¢„çƒ­æœŸã€‚

3. The traces appear as a *random walk* around the space, that is, the paths exhibit correlation with previous positions. This is both good and bad. We will always have correlation between current positions and the previous positions, but too much of it means we are not exploring the space well. This will be detailed in the Diagnostics section later below.
>è¿™äº›è¿¹çœ‹èµ·æ¥åƒæ˜¯åœ¨å›´ç»•ç©ºé—´ä¸­æŸä¸€åŒºåŸŸéšæœºæ¸¸èµ°ã€‚è¿™å°±æ˜¯è¯´å®ƒæ€»æ˜¯åœ¨åŸºäºŽä¹‹å‰çš„ä½ç½®ç§»åŠ¨ã€‚è¿™æ ·çš„å¥½å¤„æ˜¯ç¡®ä¿äº†å½“å‰ä½ç½®ä¸Žä¹‹å‰ä½ç½®ä¹‹é—´å­˜åœ¨ç›´æŽ¥ã€ç¡®å®šçš„è”ç³»ã€‚ç„¶è€Œåå¤„å°±æ˜¯å¤ªè¿‡äºŽé™åˆ¶æŽ¢ç´¢ç©ºé—´çš„æ•ˆçŽ‡ã€‚



To achieve further convergence, we will perform *more* MCMC steps. In the pseudo-code algorithm of MCMC above, the only position that matters is the current position (new positions are investigated near the current position), implicitly stored as part of the `trace` object. To continue where we left off, we pass the `trace` that we have already stored into the `sample()` function with the same step value. The values that we have already calculated will not be overwritten. This ensures that our sampling continues where it left off in the same way that it left off. 

We will sample the MCMC ***fifty thousand*** *more* times and visualize progress below:
```python
with model:
    trace = pm.sample(50000, step=[step1, step2], trace=trace)
```
```python
center_trace = trace["centers"][25000:]
prev_center_trace = trace["centers"][:25000]
center_trace.shape
```
```python
center_trace[:,1].shape
```
```python
figsize(12.5, 4)

x = np.arange(25000)
plt.plot(x, prev_center_trace[:, 0], label="previous trace of center 0",
     lw=lw, alpha=0.4, c=colors[1])
plt.plot(x, prev_center_trace[:, 1], label="previous trace of center 1",
     lw=lw, alpha=0.4, c=colors[0])

x = np.arange(25000, 75000)
plt.plot(x, center_trace[:50000, 0], label="new trace of center 0", lw=lw, c="#348ABD")
plt.plot(x, center_trace[:50000, 1], label="new trace of center 1", lw=lw, c="#A60628")

plt.title("Traces of unknown center parameters")
leg = plt.legend(loc="upper right")
leg.get_frame().set_alpha(0.8)
plt.xlabel("Steps");
```
**4. å¦‚ä½•ä¼°è®¡å„ä¸ªæœªçŸ¥å˜é‡çš„æœ€ä½³åŽéªŒä¼°è®¡å€¼**

Our main challenge is to identify the clusters. 

We have determined posterior distributions for our unknowns. We plot the posterior distributions of the center and standard deviation variables below:
```python
figsize(11.0, 4)
std_trace = trace["sds"][25000:]
prev_std_trace = trace["sds"][:25000]

_i = [1, 2, 3, 4]
for i in range(2):
    plt.subplot(2, 2, _i[2 * i])
    plt.title("Posterior of center of cluster %d" % i)
    plt.hist(center_trace[:, i], color=colors[i], bins=30,
             histtype="stepfilled")

    plt.subplot(2, 2, _i[2 * i + 1])
    plt.title("Posterior of standard deviation of cluster %d" % i)
    plt.hist(std_trace[:, i], color=colors[i], bins=30,
             histtype="stepfilled")
    # plt.autoscale(tight=True)

plt.tight_layout()
```
![](https://markpersonal.oss-us-east-1.aliyuncs.com/pic/20200313165910.png)

The MCMC algorithm has proposed that the most likely centers of the two clusters are near 120 and 200 respectively. The most likely standard deviation is 50 for cluster 0, and 22 for cluster 1.

We are also given the posterior distributions for the labels of the data point, which is present in `trace["assignment"]`. Below is a visualization of this. The y-axis represents a subsample of the posterior labels for each data point. The x-axis are the sorted values of the data points. A red square is an assignment to cluster 1, and a blue square is an assignment to cluster 0. 
```python
import matplotlib as mpl
figsize(10, 10)
plt.cmap = mpl.colors.ListedColormap(colors)
plt.imshow(trace["assignment"][::400, np.argsort(data)],
       cmap=plt.cmap, aspect=.4, alpha=.9)
plt.xticks(np.arange(0, data.shape[0], 40),
       ["%.2f" % s for s in np.sort(data)[::40]])
plt.ylabel("posterior sample")
plt.xlabel("value of $i$th data point")
plt.title("Posterior labels of data points");
```
![](https://markpersonal.oss-us-east-1.aliyuncs.com/pic/20200313170110.png)
Looking at the above plot, it appears that the most uncertainty is between 150 and 170. The above plot slightly misrepresents things, as the x-axis is not a true scale (it displays the value of the $i$th sorted data point). A more clear diagram is below, where we estimate the *frequency* of each data point belonging to labels 0 and 1:
```python
figsize(10, 5)
cmap = mpl.colors.LinearSegmentedColormap.from_list("BMH", colors)
assign_trace = trace["assignment"]
plt.scatter(data, 1 - assign_trace.mean(axis=0), cmap=cmap,
        c=assign_trace.mean(axis=0), s=50)
plt.ylim(-0.05, 1.05)
plt.xlim(35, 300)
plt.title("Probability of data point belonging to cluster 0")
plt.ylabel("probability")
plt.xlabel("value of data point");
```
![](https://markpersonal.oss-us-east-1.aliyuncs.com/pic/20200313170246.png)
Even though we modeled the clusters using Normal distributions, we didn't get just two Normal distributions that *best* fit the data, but a distribution of values for the Normal distributions' parameters. 

>Repeat this twice more, because it's *key* to Bayesian estimation.

å¯ä»¥çœ‹åˆ°ï¼Œè™½ç„¶å¯¹æ­£æ€åˆ†å¸ƒå¯¹ä¸¤ç±»æ•°æ®è¿›è¡Œäº†å»ºæ¨¡ï¼ŒMCMCä¹Ÿæ ¹æ®è§‚æµ‹æ ·æœ¬å¾—åˆ°äº†æœªçŸ¥å˜é‡çš„åŽéªŒæ¦‚çŽ‡åˆ†å¸ƒã€‚ä½†æ˜¯æˆ‘ä»¬ä»ç„¶æ²¡æœ‰å¾—åˆ°èƒ½å¤Ÿæœ€ä½³åŒ¹é…æ•°æ®çš„æ­£æ€åˆ†å¸ƒï¼Œè€Œä»…ä»…æ˜¯å¾—åˆ°äº†å…³äºŽæ­£æ€åˆ†å¸ƒå„å‚æ•°çš„åˆ†å¸ƒã€‚å½“ç„¶ï¼Œè¿™ä¹Ÿä½“çŽ°äº†è´å¶æ–¯æŽ¨æ–­çš„ä¸€ä¸ªç‰¹ç‚¹ï¼Œè´å¶æ–¯æŽ¨æ–­å¹¶ä¸ç›´æŽ¥ä½œå‡ºå†³ç­–ï¼Œå®ƒæ›´å¤šåœ°æ˜¯æä¾›çº¿ç´¢å’Œè¯æ®ï¼Œå†³ç­–è¿˜æ˜¯éœ€è¦ç»Ÿè®¡å­¦å®¶æ¥å®Œæˆã€‚

é‚£æŽ¥ä¸‹æ¥ä¸€ä¸ªå¾ˆè‡ªç„¶çš„é—®é¢˜æ˜¯ï¼Œæˆ‘ä»¬å¦‚ä½•èƒ½å¤Ÿé€‰æ‹©èƒ½å¤Ÿæ»¡è¶³æœ€ä½³åŒ¹é…çš„å‚æ•° - å‡å€¼ã€æ–¹å·®å‘¢ï¼Ÿ

**ä¸€ä¸ªç®€å•ç²—æš´çš„æ–¹æ³•æ˜¯é€‰æ‹©åŽéªŒåˆ†å¸ƒçš„å‡å€¼**ï¼ˆå½“ç„¶ï¼Œè¿™éžå¸¸åˆç†ä¸”æœ‰åšå®žçš„ç†è®ºæ”¯æ’‘ï¼‰ã€‚åœ¨ä¸‹å›¾ä¸­ï¼Œæˆ‘ä»¬ä»¥åŽéªŒåˆ†å¸ƒçš„å‡å€¼ä½œä¸ºæ­£æ€åˆ†å¸ƒçš„å„å‚æ•°å€¼ï¼Œå¹¶å°†å¾—åˆ°çš„æ­£æ€åˆ†å¸ƒäºŽè§‚æµ‹æ•°æ®å½¢çŠ¶å åŠ åˆ°ä¸€èµ·ã€‚
```python
norm = stats.norm
figsize(15, 5)
x = np.linspace(20, 300, 500)
posterior_center_means = center_trace.mean(axis=0)
posterior_std_means = std_trace.mean(axis=0)
posterior_p_mean = trace["p"].mean()

plt.hist(data, bins=20, histtype="step", normed=True, color="k",
     lw=2, label="histogram of data")
y = posterior_p_mean * norm.pdf(x, loc=posterior_center_means[0],
                                scale=posterior_std_means[0])
plt.plot(x, y, label="Cluster 0 (using posterior-mean parameters)", lw=3)
plt.fill_between(x, y, color=colors[1], alpha=0.3)

y = (1 - posterior_p_mean) * norm.pdf(x, loc=posterior_center_means[1],
                                      scale=posterior_std_means[1])
plt.plot(x, y, label="Cluster 1 (using posterior-mean parameters)", lw=3)
plt.fill_between(x, y, color=colors[0], alpha=0.3)

plt.legend(loc="upper left")
plt.title("Visualizing Clusters using posterior-mean parameters");
```
![](https://markpersonal.oss-us-east-1.aliyuncs.com/pic/20200313170621.png)
é€šè¿‡ç»“æžœå¯ä»¥çœ‹åˆ°ï¼Œå–å‡å€¼ä½œä¸ºåŽéªŒæ¯”è¾ƒå¥½çš„â€œæ‹Ÿåˆâ€äº†è§‚æµ‹æ•°æ®

**5. å›žåˆ°èšç±»ï¼šé¢„æµ‹é—®é¢˜ - åˆ°äº†è¯¥å†³ç­–çš„æ—¶å€™äº†ï¼**
The above clustering can be generalized to $k$ clusters. Choosing $k=2$ allowed us to visualize the MCMC better, and examine some very interesting plots. 

What about prediction? Suppose we observe a new data point, say $x = 175$, and we wish to label it to a cluster. It is foolish to simply assign it to the *closer* cluster center, as this ignores the standard deviation of the clusters, and we have seen from the plots above that this consideration is very important. More formally: we are interested in the *probability* (as we cannot be certain about labels) of assigning $x=175$ to cluster 1. Denote the assignment of $x$ as $L_x$, which is equal to 0 or 1, and we are interested in $P(L_x = 1 \;|\; x = 175 )$.  

A naive method to compute this is to re-run the above MCMC with the additional data point appended. The disadvantage with this method is that it will be slow to infer for each novel data point. Alternatively, we can try a *less precise*, but much quicker method. 

We will use Bayes Theorem for this:

$$ P( A | X ) = \frac{ P( X  | A )P(A) }{P(X) }$$

In our case, $A$ represents $L_x = 1$ and $X$ is the evidence we have: we observe that $x = 175$. For a particular sample set of parameters for our posterior distribution, $( \mu_0, \sigma_0, \mu_1, \sigma_1, p)$, we are interested in asking "*is the probability that $x$ is in cluster 1 **greater** than the probability it is in cluster 0*?", where the probability is dependent on the chosen parameters.

$$  P(L_x = 1| x = 175 ) \gt P(L_x = 0| x = 175 ) \;\;\;? \\\\[5pt]$$
$$  \frac{ P( x=175  | L_x = 1  )P( L_x = 1 ) }{P(x = 175) } \gt \frac{ P( x=175  | L_x = 0  )P( L_x = 0 )}{P(x = 175) } \;\;\;?$$

As the denominators are equal, they can be ignored (and good riddance, because computing the quantity $P(x = 175)$ can be difficult). 

$$  P( x=175  | L_x = 1  )P( L_x = 1 ) \gt  P( x=175  | L_x = 0  )P( L_x = 0 ) \;\;\;?$$

Let's write this equation down probabilistically, and look at its mean to get the most realistic estimate:
```python
norm_pdf = stats.norm.pdf
p_trace = trace["p"][25000:]
prev_p_trace = trace["p"][:25000]
x = 175

v = p_trace * norm_pdf(x, loc=center_trace[:, 0], scale=std_trace[:, 0]) > \
    (1 - p_trace) * norm_pdf(x, loc=center_trace[:, 1], scale=std_trace[:, 1])

print("Probability of belonging to cluster 1:", v.mean())
```
>('Probability of belonging to cluster 1:', 0.06006)

Giving us a probability instead of a label is a very useful thing. Instead of the naive 

    L = 1 if prob > 0.5 else 0

## MCMCæ”¶æ•›æ€§è®¨è®º
### Using `MAP` to improve convergence

If you rerun the sims you may notice that our results are not consistent: Perhaps your initial cluster division was more scattered, or perhaps less scattered. The problem is that our traces are a function of the *starting values* of the MCMC algorithm. å³MCMCæ˜¯åˆå§‹å€¼æ•æ„Ÿçš„ã€‚è¿™ä¹Ÿå¾ˆè‡ªç„¶ï¼ŒMCMCçš„æœç´¢è¿‡ç¨‹æ˜¯åœ¨åšå¯å‘å¼æœç´¢ï¼Œç±»ä¼¼äºŽâ€œç›²äººæ‘¸è±¡â€çš„è¿‡ç¨‹ï¼Œæ‰€ä»¥å¾ˆè‡ªç„¶åœ°ï¼Œä¸ç”¨çš„èµ·ç‚¹ï¼Œå…¶ä¹‹åŽèµ°çš„è¿¹è‡ªç„¶ä¹Ÿæ˜¯ä¸åŒçš„ã€‚

It can be shown, mathematically, that letting the MCMC run long enough, by performing many steps, the algorithm *should forget its initial position*. In fact, this is what it means to say the MCMC converged (in practice though we can never achieve total convergence). 

Hence if we observe different posterior analysis, it is likely because our MCMC has not *fully converged yet*, and we should not use samples from it yet (we should use a larger burn-in period).

In fact, poor starting values can prevent any convergence, or significantly slow it down. Ideally, we would like to have the chain start at the *peak* of our landscape, as this is exactly where the posterior distributions exist. Hence, if we started at the peak, we could avoid a lengthy burn-in period and incorrect inference. Generally, we call this *peak* the [maximum a posterior](https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation) or, more simply, the *MAP*.
**æˆ‘ä»¬å¸¸å¸¸ä¼šåœ¨æ·±åº¦å­¦ä¹ é¡¹ç›®ä¸­ï¼Œç›´æŽ¥åŸºäºŽresNetã€googleNetè¿™ç§å·²ç»ç»è¿‡è®­ç»ƒä¼˜åŒ–åŽçš„æ¨¡åž‹ï¼Œå…¶èƒŒåŽçš„æ€æƒ³ä¹Ÿæœ‰ä¸€äº›å‡å°‘é¢„çƒ­æœŸçš„æ„æ€ï¼Œåœ¨resNetã€googleNetçš„åŸºç¡€ä¸Šï¼Œåœ¨ç»§ç»­è¿›è¡Œè®­ç»ƒï¼Œå¯ä»¥æ›´å¿«åœ°æ”¶æ•›åˆ°æˆ‘ä»¬çš„ç›®æ ‡æ¦‚çŽ‡åˆ†å¸ƒä¸Šã€‚**

Of course, we do not know where the MAP is. PyMC3 provides a function that will approximate, if not find, the MAP location. In the PyMC3 main namespace is the `find_MAP` function. If you call this function within the context of `Model()`, it will calculate the MAP which you can then pass to `pm.sample()` as a `start` parameter.

    start = pm.find_MAP()
    trace = pm.sample(2000, step=pm.Metropolis, start=start)

The `find_MAP()` function has the flexibility of allowing the user to choose which optimization algorithm to use (after all, this is a optimization problem: we are looking for the values that maximize our landscape), as not all optimization algorithms are created equal. 

The default optimization algorithm in function call is the Broyden-Fletcher-Goldfarb-Shanno ([BFGS](https://en.wikipedia.org/wiki/Broyden-Fletcher-Goldfarb-Shanno_algorithm)) algorithm to find the maximum of the log-posterior. 
As an alternative, you can use other optimization algorithms from the `scipy.optimize` module. For example, you can use Powell's Method, a favourite of PyMC blogger [Abraham Flaxman](http://healthyalgorithms.com/) [1], by calling `find_MAP(fmin=scipy.optimize.fmin_powell)`. 

### Diagnosing Convergence
**Autocorrelation-è‡ªç›¸å…³ï¼ˆåºåˆ—é€’å½’æŽ¨æ¼”æ€§ï¼‰**

Autocorrelation is a measure of how related a series of numbers is with itself. A measurement of 1.0 is perfect positive autocorrelation, 0 no autocorrelation, and -1.0 is perfect negative correlation.  If you are familiar with standard *correlation*, then autocorrelation is just how correlated a series, $x_\tau$, at time $t$ is with the series at time $t-k$:

$$R(k) = Corr( x_t, x_{t-k} ) $$

>If the series is autocorrelated, you *can predict it*. If not, *you cannot*!

For example, consider the two series:

$$x_t \sim \text{Normal}(0,1), \;\; x_0 = 0$$
$$y_t \sim \text{Normal}(y_{t-1}, 1 ), \;\; y_0 = 0$$

which have example paths like:
```python
figsize(12.5, 4)

import pymc3 as pm
x_t = np.random.normal(0, 1, 200)
x_t[0] = 0
y_t = np.zeros(200)
for i in range(1, 200):
    y_t[i] = np.random.normal(y_t[i - 1], 1)

plt.plot(y_t, label="$y_t$", lw=3)
plt.plot(x_t, label="$x_t$", lw=3)
plt.xlabel("time, $t$")
plt.legend();
```
![](https://markpersonal.oss-us-east-1.aliyuncs.com/pic/20200313221304.png)

One way to think of autocorrelation is **"*if I know the position of the series at time $s$, can it help me know where I am at time $t$*?"**

 In the series $x_t$, the answer is ***No***. By construction, $x_t$ are random variables. If I told you that $x_2 = 0.5$, could you give me a better guess about $x_3$? 
***No***.

On the other hand, $y_t$ is autocorrelated. By construction, if I knew that $y_2 = 10$, I can be very confident that $y_3$ will not be very far from 10. Similarly, I can even make a (less confident guess) about $y_4$: it will probably not be near 0 or 20, but a value of 5 is not too unlikely. I can make a similar argument about $y_5$, but again, I am less confident. Taking this to it's logical conclusion, we must concede that as $k$, the lag between time points, increases, the autocorrelation decreases. We can visualize this:
```python
def autocorr(x):
    # from http://tinyurl.com/afz57c4
    result = np.correlate(x, x, mode='full')
    result = result / np.max(result)
    return result[result.size // 2:]

colors = ["#348ABD", "#A60628", "#7A68A6"]

x = np.arange(1, 200)
plt.bar(x, autocorr(y_t)[1:], width=1, label="$y_t$",
        edgecolor=colors[0], color=colors[0])
plt.bar(x, autocorr(x_t)[1:], width=1, label="$x_t$",
        color=colors[1], edgecolor=colors[1])

plt.legend(title="Autocorrelation")
plt.ylabel("measured correlation \nbetween $y_t$ and $y_{t-k}$.")
plt.xlabel("k (lag)")
plt.title("Autocorrelation plot of $y_t$ and $x_t$ for differing $k$ lags.");
```
![](https://markpersonal.oss-us-east-1.aliyuncs.com/pic/20200313221602.png)
Notice that as $k$ increases, the autocorrelation of $y_t$ decreases from a very high point. Compare with the autocorrelation of $x_t$ which looks like noise (***which it really is***), hence we can conclude no autocorrelation exists in this series. 
**è¿˜è®°å¾—HMMéšé©¬å°”ç§‘å¤«å‡è®¾å—ï¼Ÿå³å½“å‰çš„èŠ‚ç‚¹çŠ¶æ€åªå’Œä¹‹å‰æœ‰é™æ­¥éª¤ï¼ˆä¾‹å¦‚1æ­¥ï¼‰çš„èŠ‚ç‚¹çŠ¶æ€æœ‰å…³ï¼Œè™½ç„¶ç†è®ºä¸Šåº”è¯¥æ˜¯å’ŒåŽ†å²ä¸Šæ‰€æœ‰çš„èŠ‚ç‚¹çŠ¶æ€æœ‰ç›¸å…³ï¼Œä½†æ˜¯å…¶å®žè¶Šå¾€å‰ï¼Œç›¸å…³æ€§è¶Šå°ï¼Œç”šè‡³å°åˆ°å¯ä»¥å¿½ç•¥ï¼Œå› ä¸ºHMMçš„å‡è®¾å®žé™…ä¸Šå¹¶æ²¡æœ‰ä¸¢å¤±å¤ªå¤šçš„æ¦‚çŽ‡ä¿¡æ¯ã€‚**

**How does this relate to MCMC convergence?**

By the nature of the MCMC algorithm, we will always be returned samples that exhibit autocorrelation (this is because of the step `from your current position, move to a position near you`).

A chain that is ***not*** exploring the space well will exhibit very high autocorrelation. Visually, if the trace seems to meander like a river, and not settle down, the chain will have high autocorrelation.

This does not imply that a converged MCMC has low autocorrelation. Hence low autocorrelation is ***not necessary*** for convergence, but it is ***sufficient***.

>***If*** you observe low autocorrelation, your chain has converged. 

PyMC3 has a built-in autocorrelation plotting function in the `plots` module. 

**Thinning**

Another issue can arise if there is high-autocorrelation between posterior samples. Many post-processing algorithms require samples to be *independent* of each other. This can be solved, or at least reduced, by only returning to the user every $n$th sample, thus removing some autocorrelation. Below we perform an autocorrelation plot for $y_t$ with differing levels of **thinning**:
```python
max_x = 200 // 3 + 1
x = np.arange(1, max_x)

plt.bar(x, autocorr(y_t)[1:max_x], edgecolor=colors[0],
        label="no thinning", color=colors[0], width=1)
plt.bar(x, autocorr(y_t[::2])[1:max_x], edgecolor=colors[1],
        label="keeping every 2nd sample", color=colors[1], width=1)
plt.bar(x, autocorr(y_t[::3])[1:max_x], width=1, edgecolor=colors[2],
        label="keeping every 3rd sample", color=colors[2])

plt.autoscale(tight=True)
plt.legend(title="Autocorrelation plot for $y_t$", loc="lower left")
plt.ylabel("measured correlation \nbetween $y_t$ and $y_{t-k}$.")
plt.xlabel("k (lag)")
plt.title("Autocorrelation of $y_t$ (no thinning vs. thinning) \
at differing $k$ lags.");
```
![](https://markpersonal.oss-us-east-1.aliyuncs.com/pic/20200313222020.png)

With more thinning, the autocorrelation drops quicker. There is a tradeoff though: higher thinning requires more MCMC iterations to achieve the same number of returned samples. For example, 10 000 samples unthinned is 100 000 with a thinning of 10 (though the latter has less autocorrelation). 

What is a good amount of thinning? The returned samples will always exhibit some autocorrelation, regardless of how much thinning is done. So long as the autocorrelation tends to zero, you are probably ok. Typically thinning of more than 10 is not necessary.

Artificial Neural Network algorithms have a simliar concept, called **minibatch**:
- If one updates model parameters after processing the whole training data (i.e., epoch), it would take too long to get a model update in training, and the entire training data probably wonâ€™t fit in the memory.
- If one updates model parameters after processing every instance (i.e., stochastic gradient descent), model updates would be too noisy, and the process is not computationally efficient.
- Therefore, minibatch gradient descent is introduced as a trade-off between {fast model updates, memory efficiency} and {accurate model updates, computational efficiency}.

### `pymc3.plots`

It is not necessary to manually create histograms, autocorrelation plots and trace plots each time we perform MCMC. The authors of PyMC3 have included a visualization tool for just this purpose. 

The `pymc3.plots` module contains a few different plotting functions that you might find useful. For each different plotting function contained therein, you simply pass a `trace` returned from sampling as well as a list, `varnames`, of the variables that you are interested in. This module can provide you with plots of autocorrelation and the posterior distributions of each variable and their traces, among others.

Below we use the tool to plot the centers of the clusters.
```python
pm.plots.traceplot(trace, varnames=["centers"])
pm.plots.plot_posterior(trace["centers"][:,0])
pm.plots.plot_posterior(trace["centers"][:,1])
pm.plots.autocorrplot(trace, varnames=["centers"]);
```
The first plotting function gives us the posterior density of each unknown in the `centers` variable as well as the `trace` of each. `trace` plot is useful for inspecting that possible "*meandering*" property that is a result of non-convergence. The density plot gives us an idea of the shape of the distribution of each unknown, but it is better to look at each of them individually.

The second plotting function(s) provides us with a histogram of our model parameter samples with a few added features:
- The text overlay in the center shows us the posterior mean, which is a good summary of posterior distribution. 
- The interval marked by the horizontal black line overlay represents the *95% credible interval*, sometimes called the *highest posterior density interval* and not to be confused with a *95% confidence interval* (CI) of frequentist statistics. The *highest posterior density interval* can be interpreted as "*there is a 95% chance the parameter of interest lies in this interval*". When communicating your results to others, it is important to state this interval. 

One of the purposes for studying Bayesian methods is to have a clear understanding of our uncertainty in the model parameters. Combined with the posterior mean (the best "*guess*" for model parameters), the 95% credible interval provides a reliable interval to communicate the likely location of the best guess *and* the uncertainty (represented by the width of the interval).

> The *confidence interval* is a frequentist statistics construct. A confidence level represents the frequency (i.e. the proportion) of possible confidence intervals that contain the true value of the unknown population parameter. In other words, if confidence intervals are constructed using a given confidence level from an infinite number of independent sample statistics (parallel universes), the proportion of those intervals that contain the true value of the parameter will be equal to the confidence level.For example, if the confidence level (CL) is 95% then in hypothetical indefinite data collection, in 95% of the samples the interval estimate will contain the true population parameter. This is not the same as saying "*there is a 95% chance the parameter of interest lies in this interval*", and I much prefer the latter over the former!

The last plots, titled `center_0` and `center_1` are the generated autocorrelation plots, similar to the ones displayed above. Decreasing autocorrelation is a sufficient condition for convergence, so it gives us a good warm fuzzy we achieved convergence.

## Useful tips for MCMC

Bayesian inference would be the *de facto* method if it weren't for MCMC's computational difficulties (especially of the denominator). In fact, MCMC is what turns most users off practical Bayesian inference. Below are some good heuristics  to help convergence and speed up the MCMC engine. This is all part of the *art* of Bayesian estimation.

### Intelligent starting values

åˆå§‹é€‰æ‹©åœ¨åŽéªŒæ¦‚çŽ‡é™„è¿‘ï¼Œè¿™æ ·èŠ±å¾ˆå°‘çš„æ—¶é—´å°±å¯ä»¥è®¡ç®—å‡ºæ­£ç¡®ç»“æžœã€‚We can aid the algorithm by telling where we *think* the posterior distribution will be by specifying the `testval` parameter in the `Stochastic` variable creation. In many cases we can produce a reasonable guess for the parameter. For example, if we have data from a Normal distribution, and we wish to estimate the $\mu$ parameter, then a good starting value would be the *mean* of the data. 

     mu = pm.Uniform( "mu", 0, 100, testval = data.mean() )

For most parameters in models, there is a frequentist estimate of it. These estimates are a good starting value for our MCMC algorithms. Of course, this is not always possible for some variables, but including as many appropriate initial values is always a good idea. Even if your guesses are wrong, the MCMC will still converge to the proper distribution, so there is little to lose.

This is what using `MAP` tries to do, by giving good initial values to the MCMC. So why bother specifying user-defined values? Well, even giving `MAP` good values will help it find the maximum a-posterior. 

Also important, *bad initial values* are a source of major bugs in PyMC3 and can hurt convergence.

### Good Priors

If the priors are poorly chosen, the MCMC algorithm may not converge, or at least have difficulty converging. Consider what may happen if the prior chosen does not even contain the true parameter: the prior assigns 0 probability to the unknown, hence the posterior will assign 0 probability as well. This can cause pathological results.

For this reason, it is best to *carefully* choose the priors. Often, lack of covergence or evidence of samples crowding to boundaries implies something is wrong with the chosen priors (see *Folk Theorem of Statistical Computing* below). 

### Covariance matrices and eliminating parameters

Minimizing the number of parameters in your model and especially parameters that are interdependent is an important consideration. You can use covariance matrices to see if parameters turn out to be very dependent. If so, return to the drawing board and build a lower-dimensional model with statistically independent parameters.

### The Folk Theorem of Statistical Computing

>   *If you are having computational problems Ilike getting many `NaN`s, probably your model is wrong.*
