# Bayesian methods for determining model parameters

>Reference: 
>- [INFO-6105](./DataScience%20Reference/mcmc-mixture-model-dino-start.ipynb)
>- [æ‰“å¼€MCMCï¼ˆé©¬å°”ç§‘å¤«è’™ç‰¹å¡æ´›ï¼‰çš„é»‘ç›’å­ - Pymcè´å¶æ–¯æ¨ç†åº•å±‚å®ç°åŸç†åˆæ¢
](https://www.cnblogs.com/LittleHann/p/9550757.html#_lab2_1_1)
>- [AI pioneer Sejnowski says itâ€™s all about the gradient](https://www.zdnet.com/article/ai-pioneer-sejnowski-says-its-all-about-the-gradient/)

## The Bayesian expectation fabric

When we create a model with  ğ‘  parameters that we want to solve with Baeysian inference, we are implicitly creating an  ğ‘  dimensional spaceï¼ˆ**å¯ä»¥ç†è§£ä¸ºNä¸ªéšæœºå˜é‡**ï¼‰for the **prior** distribution of each paramater to live in.

Associated with the space is an **extra dimension**, which we can describe as the surface, or manifold, that sits on top of the space, that reflects the probability of observing data. The surface on the space is defined by our prior distribution and warped by the **data likelihood**. I call this fabric the **Bayesian expectation fabric**.

### å…ˆéªŒåˆ†å¸ƒçš„å¯è§†åŒ–å›¾æ™¯åƒ

æˆ‘ä»¬è¿™é‡Œé€‰æ‹©2ç»´çš„ï¼Œå³åŒ…å«2ä¸ªéšæœºå˜é‡çš„è´å¶æ–¯æ¨æ–­é—®é¢˜ï¼Œè¿›è¡Œå¯è§†åŒ–å±•ç¤ºï¼Œé€‰æ‹©2ç»´æ˜¯å› ä¸ºå¯ä»¥æ–¹ä¾¿è¿›è¡Œå¯è§†åŒ–ï¼Œé«˜ç»´ç©ºé—´æ˜¯å¾ˆéš¾æƒ³è±¡çš„ã€‚

**äºŒç»´å‡åŒ€åˆ†å¸ƒçš„å…ˆéªŒå›¾æ™¯åƒ**

For example, if we have two unknown probability distributions  ğ‘1  and  ğ‘2 , and priors for both are  Uniform(0,5) , the space created is a square of length 5 and the surface is a flat plane that sits on top of the square (representing the concept that every point is equally likely).
```python
%matplotlib inline
import scipy.stats as stats
from IPython.core.pylabtools import figsize
import numpy as np
figsize(12.5, 4)

import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

jet = plt.cm.jet
fig = plt.figure()
x = y = np.linspace(0, 5, 100)
X, Y = np.meshgrid(x, y)

plt.subplot(121)
uni_x = stats.uniform.pdf(x, loc=0, scale=5) #å‡åŒ€åˆ†å¸ƒ
uni_y = stats.uniform.pdf(y, loc=0, scale=5) #å‡åŒ€åˆ†å¸ƒ
M = np.dot(uni_x[:, None], uni_y[None, :])
im = plt.imshow(M, interpolation='none', origin='lower',
                cmap=jet, vmax=1, vmin=-.15, extent=(0, 5, 0, 5))

plt.xlim(0, 5)
plt.ylim(0, 5)
plt.title("Landscape formed by Uniform priors.")

ax = fig.add_subplot(122, projection='3d')
ax.plot_surface(X, Y, M, cmap=plt.cm.jet, vmax=1, vmin=-.15)
ax.view_init(azim=390)
plt.title("Uniform prior landscape; alternate view");
```
![](https://markpersonal.oss-us-east-1.aliyuncs.com/pic/20200311222736.png)

**äºŒç»´æŒ‡æ•°åˆ†å¸ƒçš„å…ˆéªŒå›¾æ™¯åƒ**

Alternatively, if the two priors are exponentials, for example  Exp(3)  and  Exp(10) , then the space is all positive numbers on the 2-D plane, and the surface induced by the priors looks like a **water-fall** that starts at the point (0,0) and flows over the positive numbers. Do you want to see what this looks like? I do.

```python
%matplotlib inline
import scipy.stats as stats
from IPython.core.pylabtools import figsize
import numpy as np
figsize(12.5, 5)

import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

jet = plt.cm.jet
fig = plt.figure()
x = y = np.linspace(0, 5, 100)
X, Y = np.meshgrid(x, y)

fig = plt.figure()
plt.subplot(121)

exp_x = stats.expon.pdf(x, scale=3)
exp_y = stats.expon.pdf(x, scale=10)
M = np.dot(exp_x[:, None], exp_y[None, :])
CS = plt.contour(X, Y, M)
im = plt.imshow(M, interpolation='none', origin='lower',
                cmap=jet, extent=(0, 5, 0, 5))
#plt.xlabel("prior on $p_1$")
#plt.ylabel("prior on $p_2$")
plt.title("$Exp(3), Exp(10)$ prior landscape")

ax = fig.add_subplot(122, projection='3d')
ax.plot_surface(X, Y, M, cmap=jet)
ax.view_init(azim=390)
plt.title("$Exp(3), Exp(10)$ prior landscape; \nalternate view");
```
![](https://markpersonal.oss-us-east-1.aliyuncs.com/pic/20200311223326.png)

The plots below visualize this. The more dark-red the color, the more prior probability is assigned to that location. Conversely, areas with darker blue represent that our priors assign very low probability to that location.ï¼ˆè¶Šçº¢ï¼Œå…ˆéªŒæ¦‚ç‡è¶Šé«˜ï¼›è¶Šè“ï¼Œå…ˆéªŒæ¦‚ç‡è¶Šä½ï¼‰

### è§‚æµ‹æ ·æœ¬æ˜¯å¦‚ä½•å½±å“æœªçŸ¥å˜é‡çš„å…ˆéªŒåˆ†å¸ƒçš„ï¼Ÿ

æ¦‚ç‡é¢æè¿°äº†æœªçŸ¥å˜é‡çš„å…ˆéªŒåˆ†å¸ƒï¼Œè€Œè§‚æµ‹æ ·æœ¬çš„ä½œç”¨æˆ‘ä»¬å¯ä»¥å½¢è±¡åœ°ç†è§£ä¸ºä¸€åªæ‰‹ï¼Œæ¯æ¬¡æ¥ä¸€ä¸ªæ ·æœ¬ï¼Œè¿™åªæ‰‹å°±æ ¹æ®è§‚æµ‹æ ·æœ¬çš„æƒ…å†µï¼Œå°†å…ˆéªŒåˆ†å¸ƒçš„æ›²é¢å‘â€œç¬¦åˆâ€è§‚æµ‹æ ·æœ¬çš„æ–¹å‘æ‹‰ä¼¸ä¸€ç‚¹ã€‚

åœ¨MCMCè¿‡ç¨‹ä¸­ï¼Œè§‚æµ‹æ ·æœ¬åªæ˜¯é€šè¿‡æ‹‰ä¼¸å’Œå‹ç¼©å…ˆéªŒåˆ†å¸ƒçš„æ›²é¢ï¼Œè®©æ›²é¢æ›´ç¬¦åˆå®é™…çš„å‚æ•°åˆ†å¸ƒï¼Œä»¥è¡¨æ˜å‚æ•°çš„çœŸå®å€¼æœ€å¯èƒ½åœ¨å“ªé‡Œã€‚

The data  ğ‘‹  changes the surface of the space by pulling and stretching the fabric of the prior surface to reflect where the true parameters likely live. More data means more pulling and stretching, and our original shape becomes mangled or insignificant compared to the newly formed shape. Less data, and our original shape is more present. Regardless, the resulting surface describes the posterior distribution.
æ•°æ®ğ‘‹è¶Šå¤šæ‹‰ä¼¸å’Œå‹ç¼©å°±è¶Šå‰å®³ï¼Œè¿™æ ·åéªŒåˆ†å¸ƒå°±å˜åŒ–çš„è¶Šå‰å®³ï¼Œå¯èƒ½å®Œå…¨çœ‹ä¸å‡ºå’Œå…ˆéªŒåˆ†å¸ƒçš„æ›²é¢æœ‰ä»€ä¹ˆå…³ç³»ï¼Œæˆ–è€…è¯´éšç€æ•°æ®çš„å¢åŠ å…ˆéªŒåˆ†å¸ƒå¯¹äºåéªŒåˆ†å¸ƒçš„å½±å“è¶Šæ¥è¶Šå°ã€‚è¿™ä¹Ÿä½“ç°äº†è´å¶æ–¯æ¨æ–­çš„æ ¸å¿ƒæ€æƒ³ï¼š**ä½ çš„å…ˆéªŒåº”è¯¥å°½å¯èƒ½åˆç†ï¼Œä½†æ˜¯å³ä½¿ä¸æ˜¯é‚£ä¹ˆçš„åˆç†ä¹Ÿæ²¡æœ‰å¤ªå¤§å…³ç³»ï¼ŒMCMCä¼šé€šè¿‡è§‚æµ‹æ ·æœ¬çš„æ‹Ÿåˆï¼Œå°½å¯èƒ½å°†å…ˆéªŒåˆ†å¸ƒè°ƒæ•´ä¸ºç¬¦åˆè§‚æµ‹æ ·æœ¬çš„åéªŒåˆ†å¸ƒã€‚**

ä½†æ˜¯å¦‚æœæ•°æ®ğ‘‹è¾ƒå°ï¼Œé‚£ä¹ˆåéªŒåˆ†å¸ƒçš„å½¢çŠ¶ä¼šæ›´å¤šçš„åæ˜ å‡ºå…ˆéªŒåˆ†å¸ƒçš„å½¢çŠ¶ã€‚åœ¨å°æ ·æœ¬çš„æƒ…å†µä¸‹ï¼ŒMCMCå¯¹åˆå§‹çš„å…ˆéªŒåˆ†å¸ƒä¼šéå¸¸æ•æ„Ÿã€‚

**ä¸åŒçš„å…ˆéªŒæ¦‚ç‡å¯¹è§‚æµ‹æ ·æœ¬è°ƒæ•´åéªŒåˆ†å¸ƒçš„é˜»åŠ›æ˜¯ä¸åŒçš„**

>**ANALOGY**: Probabilities allowed me to talk about quantum physics with you. Now Bayesian estimation will let me talk about Einstein's general relativity: Evidence warps the space of prior distributions much in the same way that planterary bodies warp space and create the force of gravity through geometry!  

![](https://markpersonal.oss-us-east-1.aliyuncs.com/pic/20200311230052.png)
æ—¶ç©ºæ›²ç‡å¼•èµ·é‡åŠ›

For two dimensions, in the opposite of gravity, data essentially pushes up the original surface to make tall mountains. 

éœ€è¦å†æ¬¡è¯´æ˜çš„æ˜¯ï¼Œåœ¨é«˜ç»´ç©ºé—´ä¸Šï¼Œæ‹‰ä¼¸å’ŒæŒ¤å‹çš„å˜åŒ–éš¾ä»¥å¯è§†åŒ–ã€‚åœ¨äºŒç»´ç©ºé—´ä¸Šï¼Œè¿™äº›æ‹‰ä¼¸ã€æŒ¤å‹çš„ç»“æœæ˜¯å½¢æˆäº†å‡ åº§å±±å³°ã€‚è€Œå½¢æˆè¿™äº›å±€éƒ¨å±±å³°çš„ä½œç”¨åŠ›ä¼šå—åˆ°å…ˆéªŒåˆ†å¸ƒçš„é˜»æŒ ã€‚

The tendency of observed data to push up the posterior probability in certain areas is checked by the prior probability distribution, **so that small in magnitude prior probability means more resistance.**

å…ˆéªŒåˆ†å¸ƒè¶Šå°æ„å‘³ç€é˜»åŠ›è¶Šå¤§ï¼›å…ˆéªŒåˆ†å¸ƒè¶Šå¤§é˜»åŠ›è¶Šå°ã€‚

>So priors and data likelhihood compete against each other, the same way your mind competes against two thoughts: "She smokes! I don't like her anymore!" competes with "but she is so pretty!"

æœ‰ä¸€ç‚¹è¦ç‰¹åˆ«æ³¨æ„ï¼Œå¦‚æœæŸå¤„çš„å…ˆéªŒåˆ†å¸ƒä¸º0ï¼Œé‚£ä¹ˆåœ¨è¿™ä¸€ç‚¹ä¸Šä¹Ÿæ¨ä¸å‡ºåéªŒæ¦‚ç‡ã€‚

Suppose the priors mentioned above represent different parameters  ğœ†  of two Poisson distributions. Now, we observe a datapoint and visualize the new landscape. This datapoint is a random variate from a 2D Poisson distribution (think about the distribution as the number of emails you recieve and the number of text messages, in one day). This new data is going to change our priors into posteriors. It is going to warp the Bayesian fabric.
```python
import numpy as np
import matplotlib.pyplot as plt
import pymc as pm
from scipy.stats.mstats import mquantiles
from separation_plot import separation_plot
import scipy.stats as stats

jet = plt.cm.jet
```
```python
# create observed data

# sample size of data we observe, trying varying this (keep it less than 100 ;)
N = 1

# the true parameters, but of course we do not see these values...
lambda_1_true = 1
lambda_2_true = 3

#...we only see the data generated, dependent on the above two values.
data = np.concatenate([
    stats.poisson.rvs(lambda_1_true, size=(N, 1)),
    stats.poisson.rvs(lambda_2_true, size=(N, 1))
], axis=1)

data
```
Now this is called the **data likelihood**, which in each dimension is the probability density function pdf (actually probability **mass** function pmf since our random variable is discrete rather than continuous) of the observed new data, which stems from a Poisson distribution, with expectation equal to the datapoint we observe (not the expectation of our priors, which is different):
```python
# plotting details.
x = y = np.linspace(.01, 5, 100)
likelihood_x = np.array([stats.poisson.pmf(data[:, 0], _x)
                        for _x in x]).prod(axis=1)
likelihood_y = np.array([stats.poisson.pmf(data[:, 1], _y)
                        for _y in y]).prod(axis=1)
L = np.dot(likelihood_x[:, None], likelihood_y[None, :])
```
![](https://markpersonal.oss-us-east-1.aliyuncs.com/pic/20200311231745.png)

First, let's plot our priors: How we think the data will be distributed, before we actually observe any data. We start with **uniform** priors:

```python
uni_x = stats.uniform.pdf(x, loc=0, scale=5)
uni_y = stats.uniform.pdf(y, loc=0, scale=5)
M = np.dot(uni_x[:, None], uni_y[None, :])
im = plt.imshow(M, interpolation='none', origin='lower',
                cmap=jet, vmax=1, vmin=-.15, extent=(0, 5, 0, 5))
plt.xlim(0, 5)
plt.ylim(0, 5)
plt.title("Landscape formed by Uniform priors on $p_1, p_2$.")
```
![](https://markpersonal.oss-us-east-1.aliyuncs.com/pic/20200311231950.png)

Our **posterior**, as given by Bayes' formula (we omit the denominator) is: The **data likelihood** L times the **prior** M. The mountain's peak is the datapoint we observe, because once we observe a datapoint, we expect all other datapoints to appear *at the same location*. We also plot, in yellow, the ***real*** expectation of the data generating mechanism, which we know is a Poisson with expectation (1, 3), and in green the datapoint we observe.
```python
plt.contour(x, y, M * L)
im = plt.imshow(M * L, interpolation='none', origin='lower',
                cmap=jet, extent=(0, 5, 0, 5))
plt.title("Landscape warped by %d data observation;\n Uniform priors on $p_1, p_2$." % N)
plt.scatter(lambda_2_true, lambda_1_true, c="y", s=50, edgecolor="none")
plt.xlim(0, 5)
plt.ylim(0, 5)
```
![](https://markpersonal.oss-us-east-1.aliyuncs.com/pic/20200311232227.png)

Given the new datapoint, we now expect to observe data coming in with probability contours as pictured above: We expect new data to come in with highest probability exactly where the previous datapoint came in.

Now, what if we had started with **exponential** priors instead? In other words, we think there is a ***data fountain*** at the ***origin***, and we expected our data to come from there instead, rather than uniformly. Then our prior becomes:
```python
exp_x = stats.expon.pdf(x, loc=0, scale=3)
exp_y = stats.expon.pdf(x, loc=0, scale=10)
M = np.dot(exp_x[:, None], exp_y[None, :])

plt.contour(x, y, M)
im = plt.imshow(M, interpolation='none', origin='lower',
                cmap=jet, extent=(0, 5, 0, 5))
plt.xlim(0, 5)
plt.ylim(0, 5)
plt.title("Landscape formed by Exponential priors on $p_1, p_2$.")
```
![](https://markpersonal.oss-us-east-1.aliyuncs.com/pic/20200311232426.png)

That is how we would expect data to flow if it flowed from the origin!

Now, we observe a datapoint! Yay! We're not alone in the universe anymore :-)

How does this datapoint modify our posterior M * L?
```python
plt.contour(x, y, M * L)
im = plt.imshow(M * L, interpolation='none', origin='lower',
                cmap=jet, extent=(0, 5, 0, 5))

plt.scatter(lambda_2_true, lambda_1_true, c="y", s=50, edgecolor="none")
plt.title("Landscape warped by %d data observation;\n Exponential priors on \
$p_1, p_2$." % N)
plt.xlim(0, 5)
plt.ylim(0, 5);
```
![](https://markpersonal.oss-us-east-1.aliyuncs.com/pic/20200311232733.png)

We see that our datapoint created a mountain peak in our expectation fabric, just like before,but more *squished*. The top of that mountain is not the datapoint itself because our prior was a data fountain at the origin, it *fights* with our datapoint and the result of that tectonic shift is the landscape you see pictured above.

Let's bring this all together into a *combined* plot, and introduce more datapoints! It's like playing God with the tectonic plates on earth! 

We color in red our datapoints, in green the expectation which is nothing more than the $\lambda$s of our Poissons. You should be able to see that as you increase the number of datapoints, your posterior gets tighter and tighter (and taller and taller) and the peak gets closer and closer to the theoretical expecation, which is where the green point lies.

The more data we observe (from a theoretical Poission distribution will well know Expectation), the tighter and tighter our mountain centered around that expectation. We've already seen this phenomenon with coin tosses!
```python
import numpy as np
import matplotlib.pyplot as plt
import pymc as pm
from scipy.stats.mstats import mquantiles
from separation_plot import separation_plot
import scipy.stats as stats

jet = plt.cm.jet

#
# create observed data
#

# sample size of data we observe, trying varying this (keep it less than 100 ;)
N = 1

# the true parameters, but of course we do not see these values...
lambda_1_true = 1
lambda_2_true = 3

#...we see the data generated, dependent on the above two values.
data = np.concatenate([
    stats.poisson.rvs(lambda_1_true, size=(N, 1)),
    stats.poisson.rvs(lambda_2_true, size=(N, 1))
], axis=1)
print(data)

#
# Likelihood function fron the observed data
#

x = y = np.linspace(.01, 5, 100)
likelihood_x = np.array([stats.poisson.pmf(data[:, 0], _x)
                        for _x in x]).prod(axis=1)
likelihood_y = np.array([stats.poisson.pmf(data[:, 1], _y)
                        for _y in y]).prod(axis=1)
L = np.dot(likelihood_x[:, None], likelihood_y[None, :])

#
# plots
#

figsize(12.5, 12)
plt.subplot(221)
uni_x = stats.uniform.pdf(x, loc=0, scale=5)
uni_y = stats.uniform.pdf(x, loc=0, scale=5)
M = np.dot(uni_x[:, None], uni_y[None, :])
im = plt.imshow(M, interpolation='none', origin='lower',
                cmap=jet, vmax=1, vmin=-.15, extent=(0, 5, 0, 5))
plt.xlim(0, 5)
plt.ylim(0, 5)
plt.title("Landscape formed by Uniform priors on $p_1, p_2$.")

plt.subplot(223)
plt.contour(x, y, M * L)
im = plt.imshow(M * L, interpolation='none', origin='lower',
                cmap=jet, extent=(0, 5, 0, 5))
plt.title("Landscape warped by %d data observation;\n Uniform priors on $p_1, p_2$." % N)
plt.scatter(lambda_2_true, lambda_1_true, c="g", s=50, edgecolor="none")
for _ in range(N):
    plt.scatter(data[_][0], data[_][1], c="r", s=50, edgecolor="none")
plt.xlim(0, 5)
plt.ylim(0, 5)

plt.subplot(222)
exp_x = stats.expon.pdf(x, loc=0, scale=3)
exp_y = stats.expon.pdf(x, loc=0, scale=10)
M = np.dot(exp_x[:, None], exp_y[None, :])

plt.contour(x, y, M)
im = plt.imshow(M, interpolation='none', origin='lower',
                cmap=jet, extent=(0, 5, 0, 5))
plt.xlim(0, 5)
plt.ylim(0, 5)
plt.title("Landscape formed by Exponential priors on $p_1, p_2$.")

plt.subplot(224)
# This is the likelihood times prior, that results in the posterior.
plt.contour(x, y, M * L)
im = plt.imshow(M * L, interpolation='none', origin='lower',
                cmap=jet, extent=(0, 5, 0, 5))

plt.scatter(lambda_2_true, lambda_1_true, c="g", s=50, edgecolor="none")
for _ in range(N):
    plt.scatter(data[_][0], data[_][1], c="r", s=50, edgecolor="none")
plt.title("Landscape warped by %d data observation;\n Exponential priors on \
$p_1, p_2$." % N)
plt.xlim(0, 5)
plt.ylim(0, 5);
```
![](https://markpersonal.oss-us-east-1.aliyuncs.com/pic/20200311234027.png)

The highest points on the mountain, corresponding the the darkest red, are biased towards (0,0) in the exponential case, which is the result from the exponential prior putting more prior weight in the (0,0) corner.

The green dot represents the true parameters, or **expectation**, and ***as we increase the number of datapoints, our mountain peak gets closer and closer to the expectation***.

å››å¼ å›¾é‡Œçš„ç»¿ç‚¹ä»£è¡¨å‚æ•°çš„çœŸå®å–å€¼ã€‚æ¯åˆ—çš„ä¸Šå›¾è¡¨ç¤ºå…ˆéªŒåˆ†å¸ƒå›¾å½¢ï¼Œä¸‹å›¾è¡¨ç¤ºåéªŒåˆ†å¸ƒå›¾å½¢ã€‚

æˆ‘ä»¬ä¸»è¦åˆ°ï¼Œè™½ç„¶è§‚æµ‹å€¼ç›¸åŒï¼Œä¸¤ç§å…ˆéªŒå‡è®¾ä¸‹å¾—åˆ°çš„åéªŒåˆ†å¸ƒå´æœ‰ä¸åŒçš„å›¾å½¢ã€‚

æˆ‘ä»¬ä»ä¸Šå›¾é‡Œæ³¨æ„åˆ°2ä¸ªç»†èŠ‚ï¼š

1. å³ä¸‹æ–¹çš„æŒ‡æ•°å…ˆéªŒå¯¹åº”çš„åéªŒåˆ†å¸ƒå›¾å½¢ä¸­ï¼Œå³ä¸Šè§’åŒºåŸŸçš„å–å€¼å¾ˆä½ï¼ŒåŸå› æ˜¯å‡è®¾çš„æŒ‡æ•°å…ˆéªŒåœ¨è¿™ä¸€åŒºåŸŸçš„å–å€¼ä¹Ÿè¾ƒä½ã€‚
2. å¦ä¸€æ–¹é¢ï¼Œå·¦ä¸‹æ–¹çš„å‡åŒ€åˆ†å¸ƒå¯¹åº”çš„åéªŒåˆ†å¸ƒå›¾å½¢ä¸­ï¼Œå³ä¸Šè§’åŒºåŸŸçš„å–å€¼ç›¸å¯¹è¾ƒé«˜ï¼Œè¿™ä¹Ÿæ˜¯å› ä¸ºå‡åŒ€å…ˆéªŒåœ¨è¯¥åŒºåŸŸçš„å–å€¼ç›¸æ¯”æŒ‡æ•°å…ˆéªŒæ›´é«˜ã€‚
3. åœ¨å³ä¸‹è§’æŒ‡æ•°åˆ†å¸ƒçš„åéªŒå›¾å½¢ä¸­ï¼Œæœ€é«˜çš„å±±å³°ï¼Œä¹Ÿå°±æ˜¯çº¢è‰²æœ€æ·±çš„åœ°æ–¹ï¼Œå‘ï¼ˆ0ï¼Œ0ï¼‰ç‚¹åæ–œï¼ŒåŸå› å°±æ˜¯æŒ‡æ•°å…ˆéªŒåœ¨è¿™ä¸ªè§’è½çš„å–å€¼æ›´é«˜ã€‚

>æ€è€ƒï¼šè¿™ä¸ªç°è±¡å…¶å®å’Œæ·±åº¦å­¦ä¹ é‡Œsigmoidå‡½æ•°çš„è°ƒæ•´è¿‡ç¨‹æ˜¯ç±»ä¼¼çš„ï¼Œsigmoidåœ¨è¶Šé è¿‘0æˆ–1æ¦‚ç‡çš„åŒºåŸŸä¸­ï¼Œè°ƒæ•´çš„é€Ÿç‡ä¼šè¶Šæ¥è¶Šæ…¢ï¼Œå³æ­»èƒ¡åŒæ•ˆåº”ã€‚å› ä¸ºè¿™æ—¶å€™sigmoidä¼šè®¤ä¸ºæ”¶æ•›å·²ç»æ¥è¿‘å°¾å£°ï¼Œè¦å‡ç¼“è°ƒæ•´çš„å¹…åº¦ï¼Œä»¥ç¨³å®šå·²ç»æ”¶æ•›åˆ°çš„ç»“æœã€‚

Try changing the sample size to other values (try 5, 10, 100) and observe how the mountain posterior changes.

That is the essence of Bayesian dynamics.

### ä½¿ç”¨ PyMC3 MCMCæ¥æœç´¢å›¾æ™¯åƒ

Exploring the deformed posterior space generated by our prior surface and observed data is a great exercise. However, we cannot naively search the space: Traversing $N$-dimensional space is exponentially difficult in $N$: the size of the space quickly blows-up as we increase $N$ ([the curse of dimensionality](http://en.wikipedia.org/wiki/Curse_of_dimensionality)).

How do we find these hidden mountains? The idea behind Markov Chain Monte Carlo algorithms is to perform an ***intelligent search*** of the space. 
æœç´¢ä»€ä¹ˆå‘¢ï¼Ÿä¸€ä¸ªç‚¹å—ï¼Ÿè‚¯å®šä¸æ˜¯ï¼Œè´å¶æ–¯æ€æƒ³çš„æ ¸å¿ƒå°±æ˜¯ä¸–ç•Œä¸Šæ²¡æœ‰100%ç¡®å®šçš„ä¸œè¥¿ï¼Œæ‰€æœ‰çš„æ¨æ–­éƒ½æ˜¯ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒã€‚

MCMC algorithms like **Metropolis** return **samples** from the posterior distribution, not the distribution itself. 

MCMC performs a task similar to repeatedly asking "*How likely is this pebble I found to be from the mountain I am searching for*?", and completes its task by returning thousands of accepted pebbles in hopes of reconstructing the original mountain. In MCMC and PyMC3 lingo, the returned sequence of "*pebbles*" are the **samples**, cumulatively called the **traces**. 
åœ¨MCMCå’ŒPyMCçš„æœ¯è¯­é‡Œï¼Œè¿™äº›è¿”å›åºåˆ—é‡Œçš„â€œçŸ³å¤´â€å°±æ˜¯è§‚æµ‹æ ·æœ¬ï¼Œç´¯è®¡èµ·æ¥ç§°ä¹‹ä¸ºâ€œè¿¹â€ã€‚

æˆ‘ä»¬å¸Œæœ›MCMCæœç´¢çš„ä½ç½®èƒ½æ”¶æ•›åˆ°åéªŒæ¦‚ç‡æœ€é«˜çš„åŒºåŸŸï¼ˆæ³¨æ„ä¸æ˜¯ä¸€ä¸ªç¡®å®šçš„ç‚¹ï¼Œæ˜¯ä¸€ä¸ªåŒºåŸŸï¼‰ã€‚ä¸ºæ­¤ï¼ŒMCMCæ¯æ¬¡éƒ½ä¼šæ¢ç´¢é™„è¿‘ä½ç½®ä¸Šçš„æ¦‚ç‡å€¼ï¼Œå¹¶æœç€æ¦‚ç‡å€¼å¢åŠ çš„æ–¹å‘å‰è¿›ã€‚
MCMC does this by exploring nearby positions and moving into areas with higher probability, picking up samples from that area.

**Why do we pick up thousands of samples?**

æˆ‘ä»¬å¯èƒ½ä¼šè¯´ï¼Œç®—æ³•æ¨¡å‹è®­ç»ƒçš„ç›®çš„ä¸å°±æ˜¯ä¸ºäº†è·å¾—æˆ‘ä»¬å¯¹éšæœºå˜é‡çš„æœ€ä¼˜ä¼°è®¡å—ï¼Ÿæ¯•ç«Ÿåœ¨å¾ˆå¤šæ—¶å€™ï¼Œæˆ‘ä»¬è®­ç»ƒå¾—åˆ°çš„æ¨¡å‹ä¼šç”¨äºä¹‹åçš„é¢„æµ‹ä»»åŠ¡ã€‚ä½†æ˜¯è´å¶æ–¯å­¦æ´¾ä¸è¿™ä¹ˆåšï¼Œè´å¶æ–¯æ¨æ–­çš„ç»“æœæ›´åƒæ˜¯ä¸€ä¸ªå‚è°‹ï¼Œå®ƒåªæä¾›ä¸€ä¸ªå»ºè®®ï¼Œè€Œæœ€åçš„å†³ç­–éœ€è¦æˆ‘ä»¬è‡ªå·±æ¥å®Œæˆï¼ˆä¾‹å¦‚æˆ‘ä»¬é€šè¿‡å–åéªŒä¼°è®¡çš„å‡å€¼ä½œä¸ºæœ€å¤§åéªŒä¼°è®¡çš„ç»“æœï¼‰

å›åˆ°MCMCçš„è®­ç»ƒè¿”å›ç»“æœï¼Œå®ƒè¿”å›æˆåƒä¸Šä¸‡çš„æ ·æœ¬è®©äººè§‰å¾—è¿™æ˜¯ä¸€ç§ä½æ•ˆçš„æè¿°åéªŒæ¦‚ç‡çš„æ–¹å¼ã€‚å®é™…ä¸Šè¿™æ˜¯ä¸€ç§éå¸¸é«˜æ•ˆçš„æ–¹æ³•ã€‚ä¸‹é¢æ˜¯å…¶å®ƒå¯èƒ½ç”¨äºè®¡ç®—åéªŒæ¦‚ç‡çš„æ–¹æ³•

1. ç”¨è§£æè¡¨è¾¾å¼æè¿°â€œå±±å³°åŒºåŸŸâ€(åéªŒåˆ†å¸ƒ)ï¼Œè¿™éœ€è¦æè¿°å¸¦æœ‰å±±å³°å’Œå±±è°·çš„ N ç»´æ›²é¢ã€‚åœ¨æ•°å­¦ä¸Šæ˜¯æ¯”è¾ƒå›°éš¾çš„ã€‚
2. ä¹Ÿå¯ä»¥è¿”å›å›¾å½¢é‡Œçš„é¡¶å³°ï¼Œè¿™ç§æ–¹æ³•åœ¨æ•°å­¦ä¸Šæ˜¯å¯è¡Œçš„ï¼Œä¹Ÿå¾ˆå¥½ç†è§£ï¼ˆè¿™å¯¹åº”äº†å…³äºæœªçŸ¥é‡çš„ä¼°è®¡é‡Œæœ€å¯èƒ½çš„å–å€¼ï¼‰ï¼Œä½†æ˜¯è¿™ç§åšæ³•å¿½ç•¥äº†å›¾å½¢çš„å½¢çŠ¶ï¼Œè€Œæˆ‘ä»¬çŸ¥é“ï¼Œåœ¨ä¸€äº›åœºæ™¯ä¸‹ï¼Œè¿™äº›å½¢çŠ¶å¯¹äºåˆ¤å®šæœªçŸ¥å˜é‡çš„åéªŒæ¦‚ç‡æ¥è¯´ï¼Œæ˜¯éå¸¸å…³é”®çš„
3. é™¤äº†è®¡ç®—åŸå› ï¼Œå¦ä¸€ä¸ªä¸»è¦åŸå› æ˜¯ï¼Œåˆ©ç”¨è¿”å›çš„å¤§é‡æ ·æœ¬å¯ä»¥åˆ©ç”¨å¤§æ•°å®šç†è§£å†³éå¸¸æ£˜æ‰‹é—®é¢˜ã€‚æœ‰äº†æˆåƒä¸Šä¸‡çš„æ ·æœ¬ï¼Œå°±å¯ä»¥åˆ©ç”¨ç›´æ–¹å›¾æŠ€æœ¯ï¼Œé‡æ„åéªŒåˆ†å¸ƒæ›²é¢ã€‚

**MCMC algorithms**

There is a large family of algorithms that perform MCMC, not just Metropolis. Most of these algorithms can be expressed at a high level as follows:

1. Start at current position.
2. Propose moving to a new position (investigate a pebble near you).
3. Accept/Reject the new position based on the position's adherence to the data and prior distributions (ask if the pebble likely came from the mountain).
4.  A.  If you accept: Move to the new position. Return to Step 1.
    B. Else: Do not move to new position. Return to Step 1. 
5. After a large number of iterations, return all accepted positions.

This way we move in the general direction towards regions where the posterior distributions live, and collect samples sparingly on the journey. Once we reach the posterior distribution, we can easily collect samples as they likely all belong to the posterior distribution. 

If the current position of the MCMC algorithm is in an area of extremely low probability, which is often the case when the algorithm begins (typically at a random location in the space), the algorithm will move in positions *that are likely not from the posterior* but better than everything else nearby. Thus the first moves of the algorithm are not reflective of the posterior. That is why we often *throw away* the first samples.

In the above algorithm, notice that only the current position matters (new positions are investigated only near the current position). This property as *memorylessness*, i.e. the algorithm does not care *how* it arrived at its current position, only that it is there. That is why the chain is **Markovian**.

### ä¸€ä¸ªMCMCæœç´¢å›¾æ™¯åƒçš„å®é™…çš„ä¾‹å­
Unsupervised Clustering using a Mixture Modelï¼ˆä½¿ç”¨æ··åˆæ¨¡å‹è¿›è¡Œæ— ç›‘ç£èšç±»ï¼‰
```python
import numpy as np
import matplotlib.pyplot as plt
import pymc as pm
import scipy.stats as stats

jet = plt.cm.jet

figsize(12.5, 4)
data = np.loadtxt("mixture_data.csv", delimiter=",")

plt.hist(data, bins=20, color="b", histtype="stepfilled", alpha=0.8)
plt.title("Histogram of the dataset")
plt.ylim([0, None]);
print(data[:10], "...")
```
![](https://markpersonal.oss-us-east-1.aliyuncs.com/pic/20200312001809.png)

It appears the data has a **bimodal** form, that is, it appears to have ***two peaks***, one near 120 and the other near 200. Perhaps there are *two clusters* within this dataset.
**èšç±»æ˜¯ä¸€ä¸ªå¾ˆå®½æ³›çš„æ¦‚å¿µï¼Œä¸ä¸€å®šä»…é™äºæˆ‘ä»¬æ‰€ç†Ÿæ‚‰çš„æ¬§å¼ç©ºé—´çš„kmeansï¼Œå®é™…ä¸Šï¼Œèšç±»ä¸ä¸€å®šéƒ½æ˜¯å‡ ä½•æ„ä¹‰ä¸Šçš„èšç±»ã€‚é€šè¿‡å¯¹åŸå§‹æ•°æ®é›†è¿›è¡Œæ¦‚ç‡åˆ†å¸ƒçš„æ‹Ÿåˆï¼Œä»è€Œè·å¾—æ•°æ®é›†ä¸­æ¯ä¸ªç‚¹æ‰€å±ç±»åˆ«çš„æ¦‚ç‡åˆ†å¸ƒï¼Œè¿™ä¹Ÿæ˜¯ä¸€ç§èšç±»ã€‚**

**é€‰æ‹©ç¬¦åˆæ•°æ®è§‚æµ‹åˆ†å¸ƒçš„æ•°å­¦æ¨¡å‹**
1. For each data point, choose cluster 1 with probability $p$, else choose cluster 2. 
2. Draw a random variate from a Normal distribution with parameters $\mu_i$ and $\sigma_i$ where $i$ was chosen in step 1.
3. Repeat.

è¿™ä¸ªç®—æ³•å¯ä»¥äº§ç”Ÿä¸è§‚æµ‹æ•°æ®ç›¸ä¼¼çš„æ•ˆæœã€‚æ‰€ä»¥é€‰æ‹©è¿™ä¸ªç®—æ³•ä½œä¸ºæ¨¡å‹ã€‚

ä½†æ˜¯ç°åœ¨çš„é—®é¢˜æ˜¯æˆ‘ä»¬ä¸çŸ¥é“å‚æ•° ğ‘ å’Œæ­£æ€åˆ†å¸ƒçš„å‚æ•°ã€‚æ‰€ä»¥è¦å­¦ä¹ æˆ–è€…æ¨æ–­å‡ºè¿™äº›æœªçŸ¥å˜é‡ã€‚

ç”¨Nor0ï¼ŒNor1åˆ†åˆ«è¡¨ç¤ºæ­£æ€åˆ†å¸ƒã€‚ä¸¤ä¸ªæ­£æ€åˆ†å¸ƒçš„å‚æ•°éƒ½æ˜¯æœªçŸ¥çš„ï¼Œå‚æ•°åˆ†åˆ«è¡¨ç¤ºä¸ºğœ‡ğ‘–,ğœğ‘–ï¼Œğ‘– = 0ï¼Œ1ã€‚